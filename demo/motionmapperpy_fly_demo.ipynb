{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bermanlabemory/motionmapperpy/blob/master/demo/motionmapperpy_fly_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbKKSrXoXFDv"
      },
      "source": [
        "The github repositories for motionmapper are -\n",
        "\n",
        "1. [MATLAB] **motionmapper** : https://github.com/gordonberman/MotionMapper\n",
        "\n",
        "2. [PYTHON] **motionmapperpy** : https://github.com/bermanlabemory/motionmapperpy/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihl-KLNKWPC-"
      },
      "source": [
        "# 1.&nbsp; Downloading and installing motionmapperpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CrqmGh9CC1g"
      },
      "source": [
        "First, we'll need to get motionmapperpy (sometimes we'll call it **mmpy** for brevity) from GitHub [![image.png](data:image/png;base64,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)](https://github.com/bermanlabemory/motionmapperpy).\n",
        "\n",
        "Below we clone this github repository, which will download a copy of the repository on this COLAB runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyYdRU07KSDB"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/bermanlabemory/motionmapperpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgBj7TeVDv6q"
      },
      "source": [
        "You'll notice the a **folder** named *motionmapperpy* created in the working directory in the left pane. This is the repository we just cloned and it contains the mmpy package and some toy datasets in the data folder. Note that our current working path is **/content/** in case you ever get lost. We still need to install motionmapperpy as a python package and we can do that by running this command -\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Change to the motionmapperpy directory we just cloned into this colab instance\n",
        "%cd motionmapperpy\n",
        "\n",
        "# Install motionmapperpy as a python package to the current python environment\n",
        "!python setup.py install\n",
        "\n",
        "# Come out of the mmpy directory.\n",
        "%cd ..\n",
        "```\n",
        "\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "*Quick note* : Colab instances come with many Python packages pre installed. You can run ```%pip list``` to see what packages are already present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYIdzKuADUu1"
      },
      "outputs": [],
      "source": [
        "## Install mmpy in this cell\n",
        "\n",
        "# Change to the motionmapperpy directory we just cloned into this colab instance\n",
        "%cd motionmapperpy\n",
        "\n",
        "# Install motionmapperpy as a python package to the current python environment\n",
        "!python setup.py install\n",
        "\n",
        "# Come out of the mmpy directory.\n",
        "%cd ..\n",
        "\n",
        "!pip3 install imageio==2.4.1\n",
        "\n",
        "# !pip install --upgrade imageio-ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uySr8CInEfKJ"
      },
      "source": [
        "\\\n",
        "\\\n",
        "\\\n",
        "Great! We should have `motionmapperpy` installed on this colab runtime now! We'll **need to restart the session** so that this notebook is able to recognize motionmapperpy (meaning we can do ```import motionmapperpy```)\n",
        "and its dependencies as python packages. We can restart the instance by going to **Runtime->Restart Session** in the **top menu bar**. It is equivalent to restarting the ipython kernel when working with Jupyter notebooks.\n",
        "\n",
        "Note that restarting the runtime does not delete files and folders we have created in this colab instance, or remove any python packages we've installed here. But doing **Disconnect and delete runtime** will do all of those things so be careful! Also, note that Google will clear our colab instance if we're not using the instance for some arbitrarily brief amount, in which case the runtime will be deleted.\n",
        "\n",
        "\n",
        "Once you have restarted the session, you are good to move on to the next section!\n",
        "\n",
        "--------------------------------------------------------------------------\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "<font size=\"10\">**Restart session before proceeding below!**</font>\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "Great! Lets now import all packages we'll use in this notebook, including motionmapperpy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feayV1ZNYDT5"
      },
      "source": [
        "# 2.&nbsp; Toy datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLb1_VPTGTyT"
      },
      "source": [
        "There are three additional small datasets present within the motionmapper repository we cloned from GitHub. These are present in **`motionmapperpy/data`** path.\n",
        "\n",
        "1. **Fly video dataset** : **fly_movie.avi** is video of a fly and **fly_movie_projections.mat** contains PCA projections obtained after segmentation and alignment of this frames/images from the movie. .\n",
        "\n",
        "2. **Mouse dataset** : **mouse_movie.avi** contains video of a mouse moving inside an arena. **mouse_movie_projections.mat** contains PCA projections obtained after segmentation and alignment of images/frames obtained from this movie.  \n",
        "\n",
        "3. **Leap tracked fly dataset** : This dataset has two movies **fly_leap_test.mp4** and **fly_leap_test_2.mp4** with 2 corresponding h5 files containing 32 points tracked using [LEAP](https://dataspace.princeton.edu/handle/88435/dsp01pz50gz79z).\n",
        "\n",
        "Try downloading the movies to check out what they look like.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "--------------------\n",
        "\n",
        "\n",
        "Alright, lets get started! We'll focus on the LEAP tracked fly dataset as our toy example in this notebook. We'll import some packages to kick things off below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HmksBlkYC7-"
      },
      "outputs": [],
      "source": [
        "# Python standard library packages to do file/folder manipulations,\n",
        "# pickle is a package to store python variables\n",
        "import glob, os, pickle, sys\n",
        "\n",
        "# time grabs current clock time and copy to safely make copies of large\n",
        "# variables in memory.\n",
        "import time, copy\n",
        "\n",
        "# datetime package is used to get and manipulate date and time data\n",
        "from datetime import datetime\n",
        "\n",
        "# this packages helps load and save .mat files older than v7\n",
        "import hdf5storage\n",
        "\n",
        "# numpy works with arrays, pandas used to work with fancy numpy arrays\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# matplotlib is used to plot and animate to make movies\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "# moviepy helps open the video files in Python\n",
        "from moviepy.editor import VideoClip, VideoFileClip\n",
        "from moviepy.video.io.bindings import mplfig_to_npimage\n",
        "\n",
        "# Scikit-learn is a go-to library in Python for all things machine learning\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# tqdm helps create progress bars in for loops\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Scipy is a go-to scientific computing library. We'll use it for median filtering.\n",
        "from scipy.ndimage import median_filter\n",
        "\n",
        "# Configuring matplotlib to show animations in a colab notebook as javascript\n",
        "# objects for easier viewing.\n",
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGMipKyinZYx"
      },
      "source": [
        "Now we can load the files associated with this dataset. We'll read the two video files using `moviepy`, and the two .h5 tracking datasets using pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6QE6dhlMxEz"
      },
      "outputs": [],
      "source": [
        "datasetnames = ['fly_leap_test', 'fly_leap_test_2']\n",
        "clips = [VideoFileClip('/content/motionmapperpy/data/fly/%s.mp4'%d) for d in datasetnames]\n",
        "\n",
        "h5s_pandas = [pd.read_hdf('motionmapperpy/data/fly/%s_positions.h5'%d) for d in datasetnames]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NacTgAlEZ8PU"
      },
      "outputs": [],
      "source": [
        "h5s_pandas[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvb0TsU-o9pj"
      },
      "source": [
        "Let's first explore some properties of the loaded movie clips and tracking data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVFvCalJpFEH"
      },
      "outputs": [],
      "source": [
        "for i, m in enumerate(clips):\n",
        "  print(f'Clip {i+1} is {m.duration} seconds long at {m.fps} fps. '\n",
        "  'The frames are {m.w} px wide and {m.h} px high.')\n",
        "print()\n",
        "for i, h5 in enumerate(h5s_pandas):\n",
        "  print(f'.h5 file {i} has shape {h5.shape}.')\n",
        "\n",
        "print('\\n\\nLeap tracked 32 points on the fly. Why do you think we have 96 dimensions?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I127x6Tpqg-"
      },
      "source": [
        "We can also use handy pandas functions ```.head()``` to look at the first few rows of the tracking dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvet0Rz_pgTX"
      },
      "outputs": [],
      "source": [
        "h5s_pandas[0].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8MT3KI4sbsT"
      },
      "source": [
        "It seems like the .h5 files have 32 bodyparts tracked, and there are 3 columns for each of the tracked bodyparts. They contain the `x` and `y` positions, as well as the *likelihood* value of the tracked position of each part.\n",
        "\n",
        "We'll ditch pandas now, and convert these datasets to numpy arrays since they're more intuitive to work with. Let's also remove all the likelihood columns in these numpy arrays (so every third column)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vVAe5xrpguP"
      },
      "outputs": [],
      "source": [
        "# We're removing every 3rd column since that contains the likelihood values\n",
        "inds_to_keep = np.mod(np.arange(1,h5.shape[1]+1),3)!=0\n",
        "print(inds_to_keep)\n",
        "\n",
        "h5s = [h5.values[:,inds_to_keep] for h5 in h5s_pandas]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5V1V0dIOOho"
      },
      "source": [
        "Lets check what the arrays look like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O7wXmmUOSFX"
      },
      "outputs": [],
      "source": [
        "print([h5.shape for h5 in h5s])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrR49UxQzvLV"
      },
      "source": [
        "To remove erratic tracking errors, we can median filter our data. We'll do that below and also reshape the data so that it is easier to work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OApbAjNccqne"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(h5s[0][:1000,:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQalJEe1zupe"
      },
      "outputs": [],
      "source": [
        "# Reshape the arrays so that they are easier to work with.\n",
        "h5s = [median_filter(x, size=(5,1)) for x in h5s]\n",
        "\n",
        "h5s = [i.reshape((-1, i.shape[1]//2, 2)) for i in h5s]\n",
        "print('New shapes : {[i.shape for i in h5s]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znSkY15gdIhz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "plt.plot(h5s[0][:1000,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvLpuxIa0GWY"
      },
      "source": [
        "Great! Now we are ready to plot and see what our dataset looks like. Below we'll use `matplotlib` to overlay tracking data on the video files. We'll read video frames using moviepy `videofileclip` objects stored in `clips`. Running this cell may take upto **1 minute**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqNz3zDXqA9C"
      },
      "outputs": [],
      "source": [
        "# This will take about about 1-2 minutes to run.\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "h5ind = 0\n",
        "tstart = 7000\n",
        "\n",
        "connections = [np.arange(6,10), np.arange(10,14), np.arange(14,18), np.arange(18,22), np.arange(22,26), np.arange(26,30),\n",
        "              [2,0,1],[0,3,4,5], [31,3,30]]\n",
        "\n",
        "try:\n",
        "  tqdm._instances.clear()\n",
        "except Exception as e:\n",
        "  pass\n",
        "\n",
        "def animate(t):\n",
        "  t = int(t*clips[h5ind].fps)+tstart\n",
        "  ax.clear()\n",
        "  for conn in connections:\n",
        "      ax.plot(h5s[h5ind][t, conn, 0], h5s[h5ind][t, conn, 1], 'k-')\n",
        "  for i in range(h5s[h5ind].shape[1]):\n",
        "      ax.scatter(h5s[h5ind][t, i,0], h5s[h5ind][t, i,1], marker=f'${i}$', s=200, color='k')\n",
        "  ax.imshow(clips[h5ind].get_frame((t)/clips[h5ind].fps), cmap='Greys', origin='lower')\n",
        "  ax.set_aspect('equal')\n",
        "  ax.axis('off')\n",
        "\n",
        "  return mplfig_to_npimage(fig)\n",
        "\n",
        "anim = VideoClip(animate, duration=20)\n",
        "plt.close()\n",
        "anim.ipython_display(fps=20, loop=True, autoplay=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIrjizpjQ3vj"
      },
      "source": [
        "## 2.1&nbsp; Data specific pre-processing\n",
        "\n",
        "[![](http://www.accutrend.com/wp-content/uploads/2018/07/GiGo-570x315.jpg)](http://www.accutrend.com/it-still-comes-down-to-garbage-in-garbage-out/)\n",
        "\n",
        "When working with any model, algorithm or pipeline, we have to be mindful of the data we are feeding into them. This section covers some (of many) tricks to process the input data before feeding into motionmapperpy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T01tFQfH0rr4"
      },
      "source": [
        "Working with the tracked bodypart positions can be challenging with flies, since there are a lot of correlations between the tracked points. We can visualize these in the plot below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWl-poK60T6M"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "_ = ax.plot(h5s[0][:1000,:].reshape((1000,-1)))\n",
        "ax.set_xlabel(\"Frame #\", fontsize=14)\n",
        "ax.set_ylabel(\"X and Y positions (px)\", fontsize=14)\n",
        "# ax.set_ylim([-0.5, 1.5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b7WvbHz1lJY"
      },
      "source": [
        "Instead, we can compute various angles using these 32 positions. Below, we'll compute 22 new angles in each frame using these positions, such as the angle between wingtips `(31, 0, 30)` in the image above.\n",
        "\n",
        "Working with angles is tricky since they will either fall between [-$\\pi$, $\\pi$] or [0, $2\\pi$], and we can have discontinuties at the boundaries. As a workaround, we can choose to compute the angles to vary over either of these two ranges. For example, the wingtips cross over very frequently so we'll choose to compute then over the [-$\\pi$, $\\pi$] interval. We'll indicate this choice by saying 0 (or 1 for the other choice) as the last list element in angleinds defined below.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkStN45BrJjN"
      },
      "outputs": [],
      "source": [
        "#convert to angles space\n",
        "\n",
        "# [1,0,2,0] means the angle from 1 to 0 to 2 calculated over [-pi, pi] interval.\n",
        "angleinds = [[1,0,2,0], [0,3,4,0], [3,4,5,0], [31,3,30,1], [6,7,8,0], [7,8,9,0], [10, 11, 12,0],\n",
        "          [11, 12, 13, 0], [14,15,16, 0] ,[15,16,17,0], [26,27,28,0] , [27,28,29,0], [22,23,24,0],\n",
        "          [23,24,25,0], [18,19,20,0], [19,20,21,0], [3,4,18,0], [3,4,22,0], [3,4,26,0], [3,4,6,0],\n",
        "          [3,4,10,0], [3,4,14,0]]\n",
        "\n",
        "# Empty array to hold computed angles.\n",
        "angleh5s = [np.zeros((h5.shape[0], len(angleinds))) for h5 in h5s]\n",
        "\n",
        "# Function to compute angle between two vectors.\n",
        "def angle_between(v1, v2, small_angle=1):\n",
        "    \"\"\"\n",
        "    Calculate angle between two vectors.\n",
        "\n",
        "    Args:\n",
        "    v1, v2 : Pair of vectors of shape (N, 2).\n",
        "    small_angle : True if calculating over [-pi, pi],\n",
        "                  False if calculating over [0, 2pi].\n",
        "\n",
        "    Returns\n",
        "    N angles in degrees.\n",
        "    \"\"\"\n",
        "    ang1 = np.arctan2(v1[:,1], v1[:,0])\n",
        "    ang2 = np.arctan2(v2[:,1], v2[:,0])\n",
        "    if small_angle:\n",
        "        out = np.rad2deg((ang1 - ang2) % (2 * np.pi))\n",
        "        out[out>180] = -1*(360-out[out>180])\n",
        "        return out\n",
        "    else:\n",
        "        return np.rad2deg((ang1 - ang2) % (2 * np.pi))\n",
        "\n",
        "\n",
        "for hi,h5 in enumerate(h5s):\n",
        "    for ai, aind in enumerate(angleinds):\n",
        "        v1 = h5[:,aind[0]]-h5[:,aind[1]]\n",
        "        v2 = h5[:,aind[2]]-h5[:,aind[1]]\n",
        "        angleh5s[hi][:,ai] = angle_between(v1, v2, small_angle=aind[3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK-GudfQ1zRY"
      },
      "source": [
        "We can look at the angles time series below. We're plotting only 5 angles here just for the sake of visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cLrahhDswM7"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "labels = ['Angle between wingtips', 'Neck angle', 'Right midleg femur-tibia angle',\n",
        "          'Left foreleg coxa-femur angle', 'Mesothorasic angle']\n",
        "for i,l in zip([3, 1, 13, 4, 2], labels):\n",
        "  _ = ax.plot(angleh5s[0][1000:2000,i]* np.pi/(180), label=l)\n",
        "ax.set_xlabel(\"Frame #\", fontsize=14)\n",
        "ax.set_ylabel(\"Angles (Rad)\", fontsize=14)\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj2CuK_62Cp8"
      },
      "source": [
        "It seems like some of these angles vary a lot and some very little. We can get rid of this asymmetry by doing a min-max scaling, where we constrain each angle to scale between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sc3yj_cEx_LZ"
      },
      "outputs": [],
      "source": [
        "# let's normalize these angles so they fall bw 0 and 1\n",
        "\n",
        "angleh5s_min = []\n",
        "for ah5 in angleh5s:\n",
        "    x = copy.deepcopy(ah5)\n",
        "    x = x - np.min([np.min(x, 0) for x in angleh5s], 0)[None, :]\n",
        "    angleh5s_min.append(x)\n",
        "\n",
        "angleh5s_normed = []\n",
        "for ah5min in angleh5s_min:\n",
        "    x = copy.deepcopy(ah5min)\n",
        "    x = x/np.max([np.max(x, 0) for x in angleh5s_min], 0)[None, :]\n",
        "    angleh5s_normed.append(x)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "labels = ['Angle between wingtips', 'Neck angle', 'Right midleg femur-tibia angle',\n",
        "          'Left foreleg coxa-femur angle', 'Mesothorasic angle']\n",
        "for i,l in zip([3, 1, 13, 4, 2], labels):\n",
        "  _ = ax.plot(angleh5s_normed[0][:1000,i], label=l)\n",
        "ax.set_xlabel(\"Frame #\", fontsize=14)\n",
        "ax.set_ylabel(\"Normalized angles\", fontsize=14)\n",
        "ax.legend()\n",
        "ax.set_ylim([0, 1.])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jruSS0bx28S2"
      },
      "source": [
        "Now, we have a 22-dimensional feature space that is normalized, and we can use our favourite tool to reduce the dimensionality further to ease up on computational costs. Here, we'll use PCA to reduce the dimensions even further.\n",
        "\n",
        "We're using the PCA implementation from [`sklearn.decomposition.PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaEJJqDgjJyp"
      },
      "outputs": [],
      "source": [
        "angleh5s_normed[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jX2VPLUAqcy3"
      },
      "outputs": [],
      "source": [
        "# Concatenate the two files for downstream analyses\n",
        "x = np.concatenate(angleh5s_normed, axis=0)\n",
        "# x.shape -> 40000, 22\n",
        "variance_threshold = 0.95\n",
        "\n",
        "#We are using sklearn.decomposition.PCA here\n",
        "p = PCA()\n",
        "y = p.fit_transform(x)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,4))\n",
        "ax.plot(np.arange(1, x.shape[1]+1), np.cumsum(p.explained_variance_ratio_), color='firebrick')\n",
        "\n",
        "#This calculates the number of PCA components required to surpass teh variance threshold\n",
        "comps_above_thresh = np.argwhere(np.cumsum(p.explained_variance_ratio_)>variance_threshold)[0][0]\n",
        "\n",
        "ax.axvline(x=comps_above_thresh, color='royalblue', linestyle='--', alpha=0.5)\n",
        "ax.text(x = comps_above_thresh + 1, y = 0.5, s=f'{comps_above_thresh}th component')\n",
        "ax.set_xlabel('PCA Components')\n",
        "ax.set_ylabel('Cumulative Explained Variance')\n",
        "ax.grid()\n",
        "\n",
        "print(f'We\\'ll pick the first {comps_above_thresh} components that explain {variance_threshold*100}% of the variance.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED8qyjD64cf7"
      },
      "source": [
        "We can look at the feature space composition of the leading principal components below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSWegNQqjWGw"
      },
      "outputs": [],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUdFBgSECS-y"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(20,4))\n",
        "\n",
        "for i in range(comps_above_thresh):\n",
        "  for pi in range(x.shape[1]):\n",
        "    ax.bar((i-0.4+pi*(0.8/x.shape[1])), p.components_[i, pi], width=(0.7/x.shape[1]), label=f'Angle{i}')\n",
        "  ax.set_prop_cycle(None)\n",
        "\n",
        "ax.set_xlabel('Leading PCA Components ')\n",
        "ax.set_ylabel('Proportion along PC direction')\n",
        "# ax.legend()\n",
        "ax.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7neVB4M5Bnc"
      },
      "source": [
        "This gives us some idea of how these maximally varying components are oriented in our high dimensional feature space.\n",
        "\n",
        "\n",
        "Now we can reduce our feature space dimensions further! Note that here we use variable `y` which was obtained after the PCA transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lkDvHo3YTuw"
      },
      "outputs": [],
      "source": [
        "#picking PCA components above threshold\n",
        "y = y[:,:comps_above_thresh]\n",
        "print(y.shape)\n",
        "\n",
        "#Let's also split y to the size of original h5 files.\n",
        "projs_list = np.split(y, np.cumsum([h5.shape[0] for h5 in h5s])[:-1])\n",
        "\n",
        "print([p.shape for p in projs_list])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJz2-Rwl6MQ5"
      },
      "source": [
        "Now we have **projs_list**, which contains two (relatively) low-dimensional timeseries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhuNfeVm4T1P"
      },
      "source": [
        "# 3.&nbsp; Creating an mmpy project directory\n",
        "\n",
        "Now that we have two low dimensional time series which **may** not set Google servers on fire, we will create our project directory for running the `motionmapperpy` pipeline on the data we have. Having a project directory is awesome, as it helps us stay organized when working with big datasets and multiple files. It allows datasets to be easily referenced and loaded without exhausting memory, and we can store pipeline outputs in well-defined and easy to read files.\n",
        "\n",
        "\n",
        "Lets start by importing `motionmapperpy` and creating a project directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smnR7O9Y4idG"
      },
      "outputs": [],
      "source": [
        "import motionmapperpy as mmpy\n",
        "%matplotlib inline\n",
        "\n",
        "projectPath = '/content/Fly_Leap_mmpy'\n",
        "\n",
        "# This creates a project directory structure which will be used to store all motionmappery pipeline\n",
        "# related data in one place.\n",
        "\n",
        "mmpy.createProjectDirectory(projectPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_uosHZN5mqS"
      },
      "source": [
        "Now lets store the two low-d time series in **projs_list** to the *`Projections`* folder in the project directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDOKMsD15_l-"
      },
      "outputs": [],
      "source": [
        "for i, projs in enumerate(projs_list):\n",
        "    hdf5storage.savemat(f'{projectPath}/Projections/{datasetnames[i]}_pcaModes.mat', {'projections': projs})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocFZeYek6gO_"
      },
      "source": [
        "We'll now go through `mmpy` parameters. They are a handful and can be overwhelming, but they are very easy to understand!\n",
        "\n",
        "Parameters are cruicial to `mmpy` as they lay out some hard-coded choices we need to make when running this pipeline. I will explain each parameter as we encounter them in the cell below, so please read through this cell below as you run it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_QuCClWcDHm"
      },
      "outputs": [],
      "source": [
        "\"\"\"2. Setup run parameters for MotionMapper.\"\"\"\n",
        "\n",
        "#% Load the default parameters.\n",
        "parameters = mmpy.setRunParameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWC2PbfYnG6X"
      },
      "outputs": [],
      "source": [
        "parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5866VHTOlTL3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# %%%%%%% PARAMETERS TO CHANGE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "# These need to be revised everytime you are working with a new dataset. #\n",
        "\n",
        "parameters.projectPath = projectPath #% Full path to the project directory.\n",
        "\n",
        "\n",
        "parameters.method = 'UMAP' #% We can choose between 'TSNE' or 'UMAP'\n",
        "\n",
        "parameters.minF = 1        #% Minimum frequency for Morlet Wavelet Transform\n",
        "\n",
        "parameters.maxF = 50       #% Maximum frequency for Morlet Wavelet Transform,\n",
        "                           #% usually equal to the Nyquist frequency for your\n",
        "                           #% measurements.\n",
        "\n",
        "parameters.samplingFreq = 100    #% Sampling frequency (or FPS) of data.\n",
        "\n",
        "parameters.numPeriods = 25       #% No. of dyadically spaced frequencies to\n",
        "                                 #% calculate between minF and maxF.\n",
        "\n",
        "parameters.pcaModes = comps_above_thresh #% Number of low-d features.\n",
        "\n",
        "parameters.numProcessors = -1     #% No. of processor to use when parallel\n",
        "                                 #% processing for wavelet calculation (if not using GPU)\n",
        "                                 #% and for re-embedding. -1 to use all cores\n",
        "                                 #% available.\n",
        "\n",
        "parameters.useGPU = -1           #% GPU to use for wavelet calculation,\n",
        "                                 #% set to -1 if GPU not present.\n",
        "\n",
        "parameters.training_numPoints = 3000    #% Number of points in mini-trainings.\n",
        "\n",
        "\n",
        "# %%%%% NO NEED TO CHANGE THESE UNLESS MEMORY ERRORS OCCUR %%%%%%%%%%\n",
        "\n",
        "parameters.trainingSetSize = 5000  #% Total number of training set points to find.\n",
        "                                 #% Increase or decrease based on\n",
        "                                 #% available RAM. For reference, 36k is a\n",
        "                                 #% good number with 64GB RAM.\n",
        "\n",
        "parameters.embedding_batchSize = 30000  #% Lower this if you get a memory error when\n",
        "                                        #% re-embedding points on a learned map.\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU8RN67i_fZe"
      },
      "source": [
        "Above covers usually relevant parameters when using `mmpy`. However, there are parameters associated with tSNE and UMAP implementations, such as below, which aren't usually required to be changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3RBAwEg_75_"
      },
      "outputs": [],
      "source": [
        "# %%%%%%% tSNE parameters %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "#% can be 'barnes_hut' or 'exact'. We'll use barnes_hut for this tutorial for speed.\n",
        "parameters.tSNE_method = 'barnes_hut'\n",
        "\n",
        "# %2^H (H is the transition entropy)\n",
        "parameters.perplexity = 32\n",
        "\n",
        "# %number of neigbors to use when re-embedding\n",
        "parameters.maxNeighbors = 200\n",
        "\n",
        "# %local neighborhood definition in training set creation\n",
        "parameters.kdNeighbors = 5\n",
        "\n",
        "# %t-SNE training set perplexity\n",
        "parameters.training_perplexity = 20\n",
        "\n",
        "\n",
        "# %%%%%%%% UMAP Parameters %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "# Size of local neighborhood for UMAP.\n",
        "n_neighbors = 15\n",
        "\n",
        "# Negative sample rate while training.\n",
        "train_negative_sample_rate = 5\n",
        "\n",
        "# Negative sample rate while embedding new data.\n",
        "embed_negative_sample_rate = 1\n",
        "\n",
        "# Minimum distance between neighbors.\n",
        "min_dist = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTh2DM5L6e8E"
      },
      "source": [
        "## 3.1&nbsp; Visualizing wavelet amplitudes\n",
        "\n",
        "This section is not required to be run by motionmapperpy, but we'll go through it to visualize spectrograms on one of the low-dimensional time series.\n",
        "\n",
        "We'll use `mmpy.findWavelets` function to obtain the waveletes, and plot the obtained spectrogram for each feature/projection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lysbx4_k61KW"
      },
      "outputs": [],
      "source": [
        "wlets, freqs = mmpy.findWavelets(projs_list[0], projs_list[0].shape[1], parameters.omega0, parameters.numPeriods, parameters.samplingFreq, parameters.maxF, parameters.minF, parameters.numProcessors, parameters.useGPU)\n",
        "\n",
        "%matplotlib inline\n",
        "fig, axes = plt.subplots(y.shape[1], 1, figsize=(20,18))\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "  ax.imshow(wlets[:300,25*i:25*(i+1)].T, cmap='PuRd', origin='lower')\n",
        "  ax.set_yticks([0, 5, 10, 15, 20, 24])\n",
        "  ax.set_yticklabels([f'{freqs[j]:0.1f}' for j in [0, 5, 10, 15, 20, 24]])\n",
        "  if i == 3:\n",
        "    ax.set_ylabel(\"Frequencies (hz)\", fontsize=14)\n",
        "  ax.set_title(f'Projection #{i+1}')\n",
        "ax.set_xlabel('Frames', fontsize=14)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWZtyxgLBWyu"
      },
      "source": [
        "As we can see, our low-d time series is soon dwarfed by the 25-dimensional wavelet amplitudes obtained for each low-d feature! This is why its wise to spend some time reducing the dimensionality of our original data, as much as we can.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aNoAnng7Cla"
      },
      "source": [
        "# 5.&nbsp; Creating a training set and embedding it using tSNE/UMAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9eJ1AzK7ybU"
      },
      "source": [
        "Even though we are working with toy datasets, we have two extremely high dimensional timeseries we're using to create a smaller and more interpretable representation. tSNE and UMAP both need to compute all-to-all distances in high-dimensional space to find neighboring points and embed them closely on this low-dimensional space we're building. This computation can quickly exhaust memory (RAM) and scale exponentially with datapoints.\n",
        "\n",
        "To navigate this challenge, we do a subsampling procedure to create a training set, and use tSNE or UMAP to create training embeddings. All of this is done in the cell below.\n",
        "\n",
        "**Time taken** : TSNE 86 sec | UMAP 44 sec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRid9ecM7q3A"
      },
      "outputs": [],
      "source": [
        "t1 = time.time()\n",
        "\n",
        "mmpy.subsampled_tsne_from_projections(parameters, parameters.projectPath)\n",
        "\n",
        "print(f'Done in {time.time()-t1} seconds.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7QH0-yOEkgZ"
      },
      "source": [
        "Note that the `training set` and `training embedding` are both save in the `project_directory/TSNE` or `project_directory/UMAP` directories depending on which method you're using. We'll load the training embedding below and plot it. You can play around with the sigma value here to change the coarseness of the density map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXfWBg7pAx2w"
      },
      "outputs": [],
      "source": [
        "trainy = hdf5storage.loadmat(f'{parameters.projectPath}/{parameters.nethod}/training_embedding.mat')['trainingEmbedding']\n",
        "m = np.abs(trainy).max()\n",
        "\n",
        "sigma=2.0\n",
        "_, xx, density = mmpy.findPointDensity(trainy, sigma, 511, [-m-20, m+20])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12,6))\n",
        "axes[0].scatter(trainy[:,0], trainy[:,1], marker='.', c=np.arange(trainy.shape[0]), s=1)\n",
        "axes[0].set_xlim([-m-20, m+20])\n",
        "axes[0].set_ylim([-m-20, m+20])\n",
        "\n",
        "axes[1].imshow(density, cmap=mmpy.gencmap(), extent=(xx[0], xx[-1], xx[0], xx[-1]), origin='lower')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-B_oiewMXAC"
      },
      "source": [
        "On the left, we see a scatter plot and on the right, we see a Gaussian kernel convolved density estimation of these points. Does it surprise you? What does changing the sigma value do?  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW5jndTkW1of"
      },
      "source": [
        "# 6.&nbsp; Finding embeddings for all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdCW8Y309RXu"
      },
      "source": [
        "Now, we can find embeddings for our entire dataset! We'll use the `mmpy.findEmbeddings` function which requires the training set and the 2-d embeddings we find in the last step, and the high-d 'projections' time series for each dataset. We'll save the obtained embeddings for each dataset neatly in the Projections folder so that we can reference them later.\n",
        "\n",
        "**Running time** : TSNE 19 mins | UMAP 3 mins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB84yPqS8u89"
      },
      "outputs": [],
      "source": [
        "#tsne takes 19 mins\n",
        "tall = time.time()\n",
        "\n",
        "import h5py\n",
        "tfolder = parameters.projectPath+'/%s/'%parameters.method\n",
        "\n",
        "# Loading training data\n",
        "with h5py.File(tfolder + 'training_data.mat', 'r') as hfile:\n",
        "    trainingSetData = hfile['trainingSetData'][:].T\n",
        "\n",
        "# Loading training embedding\n",
        "with h5py.File(tfolder+ 'training_embedding.mat', 'r') as hfile:\n",
        "    trainingEmbedding= hfile['trainingEmbedding'][:].T\n",
        "\n",
        "if parameters.method == 'TSNE':\n",
        "    zValstr = 'zVals'\n",
        "else:\n",
        "    zValstr = 'uVals'\n",
        "\n",
        "projectionFiles = glob.glob(parameters.projectPath+'/Projections/*pcaModes.mat')\n",
        "for i in range(len(projectionFiles)):\n",
        "    print('Finding Embeddings')\n",
        "    t1 = time.time()\n",
        "    print(f'{i+1}/{len(projectionFiles)} : {projectionFiles[i]}')\n",
        "\n",
        "    # Skip if embeddings already found.\n",
        "    if os.path.exists(projectionFiles[i][:-4] +f'_{zValstr}.mat'):\n",
        "        print('Already done. Skipping.\\n')\n",
        "        continue\n",
        "\n",
        "    # load projections for a dataset\n",
        "    projections = hdf5storage.loadmat(projectionFiles[i])['projections']\n",
        "\n",
        "    # Find Embeddings\n",
        "    zValues, outputStatistics = mmpy.findEmbeddings(projections,trainingSetData,trainingEmbedding,parameters)\n",
        "\n",
        "    # Save embeddings\n",
        "    hdf5storage.write(data = {'zValues':zValues}, path = '/', truncate_existing = True,\n",
        "                    filename = projectionFiles[i][:-4]+f'_{zValstr}.mat', store_python_metadata = False,\n",
        "                      matlab_compatible = True)\n",
        "\n",
        "    # Save output statistics\n",
        "    with open(projectionFiles[i][:-4] + f'_{zValstr}_outputStatistics.pkl', 'wb') as hfile:\n",
        "        pickle.dump(outputStatistics, hfile)\n",
        "\n",
        "    del zValues,projections,outputStatistics\n",
        "\n",
        "print(f'All Embeddings Saved in {time.time()-tall} seconds!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlhFOt0-G_cW"
      },
      "source": [
        "We can visualize the obtained embeddings by calling the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMHW7yDeJIy_"
      },
      "outputs": [],
      "source": [
        "# load all the embeddings\n",
        "for i in glob.glob(parameters.projectPath+f'/Projections/*_{zValstr}.mat'):\n",
        "  ally = hdf5storage.loadmat(i)['zValues']\n",
        "\n",
        "m = np.abs(ally).max()\n",
        "\n",
        "sigma=2.0\n",
        "_, xx, density = mmpy.findPointDensity(ally, sigma, 511, [-m-20, m+20])\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12,6))\n",
        "axes[0].scatter(ally[:,0], ally[:,1], marker='.', c=np.arange(ally.shape[0]), s=1)\n",
        "axes[0].set_xlim([-m-20, m+20])\n",
        "axes[0].set_ylim([-m-20, m+20])\n",
        "\n",
        "axes[1].imshow(density, cmap=mmpy.gencmap(), extent=(xx[0], xx[-1], xx[0], xx[-1]), origin='lower')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLoOcHS0OKbr"
      },
      "source": [
        "# 7.&nbsp; Watershed transform on the density map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPqF-zDYANq"
      },
      "source": [
        "There is another handy function in `motionmapperpy` called `findWatershedRegions`. This will do an iterative watershed transform on the behavioral density map until the given `minimum_regions` are found in the density map.\n",
        "\n",
        "It saves watershed transformed output of the embedding in `project_director/UMAP/zVals_wShed_groups.mat` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50zDsiEPLw4s"
      },
      "outputs": [],
      "source": [
        "startsigma = 4.2 if parameters.method == 'TSNE' else 1.0\n",
        "mmpy.findWatershedRegions(parameters, minimum_regions=10, startsigma=startsigma, pThreshold=[0.33, 0.67],\n",
        "                     saveplot=True, endident = '*_pcaModes.mat')\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(glob.glob(f'{parameters.projectPath}/{parameters.method}/zWshed*.png')[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY-yeTfGNok7"
      },
      "source": [
        "# 8.&nbsp; Ethograms and videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-m-s9GpYolw"
      },
      "source": [
        "We can now create ethograms using the watershed region time series created in the last step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IjHvBQ2Begp"
      },
      "outputs": [],
      "source": [
        "wshedfile = hdf5storage.loadmat(f'{parameters.projectPath}/{parameters.method}/zVals_wShed_groups.mat')\n",
        "\n",
        "wregs = wshedfile['watershedRegions'].flatten()\n",
        "ethogram = np.zeros((wregs.max()+1, len(wregs)))\n",
        "\n",
        "for wreg in range(1, wregs.max()+1):\n",
        "  ethogram[wreg, np.where(wregs==wreg)[0]] = 1.0\n",
        "\n",
        "\n",
        "ethogram = np.split(ethogram.T, np.cumsum(wshedfile['zValLens'][0].flatten())[:-1])\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(20,10))\n",
        "\n",
        "for e, name, ax in zip(ethogram, wshedfile['zValNames'][0], axes.flatten()):\n",
        "  print(e.shape)\n",
        "  ax.imshow(e.T, aspect='auto', cmap=mmpy.gencmap())\n",
        "  ax.set_title(name[0][0])\n",
        "  ax.set_yticks([i for i in range(1, wregs.max()+1, 4)])\n",
        "  ax.set_yticklabels([f'Region {j+1}' for j in range(1, wregs.max()+1, 4)])\n",
        "\n",
        "  xticklocs = [6000*i for i in range(3)]\n",
        "  ax.set_xticks(xticklocs)\n",
        "  ax.set_xticklabels([j/(6000) for j in xticklocs])\n",
        "\n",
        "ax.set_xlabel('Time (min)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxrwhMtMaaCa"
      },
      "source": [
        "## 8.1 Visualize behavioral map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q58dYlIODxJ"
      },
      "source": [
        "Run the below code to see the behavioral map in action.\n",
        "\n",
        "This may take **2 minutes** to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdjnI94ct-I_"
      },
      "outputs": [],
      "source": [
        "wshedfile = hdf5storage.loadmat(f'{parameters.projectPath}/{parameters.method}/zVals_wShed_groups.mat')\n",
        "\n",
        "try:\n",
        "  tqdm._instances.clear()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10,5))\n",
        "zValues = wshedfile['zValues']\n",
        "m = np.abs(zValues).max()\n",
        "\n",
        "\n",
        "sigma=1.0\n",
        "_, xx, density = mmpy.findPointDensity(zValues, sigma, 511, [-m-10, m+10])\n",
        "axes[0].imshow(density, cmap=mmpy.gencmap(), extent=(xx[0], xx[-1], xx[0], xx[-1]), origin='lower')\n",
        "axes[0].axis('off')\n",
        "axes[0].set_title('Method : %s'%parameters.method)\n",
        "sc = axes[0].scatter([],[],marker='o', color='k', s=500)\n",
        "\n",
        "h5ind = 0\n",
        "tstart = 0\n",
        "connections = [np.arange(6,10), np.arange(10,14), np.arange(14,18), np.arange(18,22), np.arange(22,26), np.arange(26,30),\n",
        "              [2,0,1],[0,3,4,5], [31,3,30]]\n",
        "\n",
        "def animate(t):\n",
        "  t = int(t*clips[h5ind].fps)+tstart\n",
        "  axes[1].clear()\n",
        "  im = axes[1].imshow(clips[h5ind].get_frame(t/clips[h5ind].fps), cmap='Greys', origin='lower')\n",
        "  for conn in connections:\n",
        "      axes[1].plot(h5s[h5ind][t, conn, 0], h5s[h5ind][t, conn, 1], 'k-')\n",
        "  axes[1].axis('off')\n",
        "  sc.set_offsets(zValues[20000*h5ind+t])\n",
        "  return mplfig_to_npimage(fig) #im, ax\n",
        "\n",
        "\n",
        "anim = VideoClip(animate, duration=2) # will throw memory error for more than 100.\n",
        "plt.close()\n",
        "anim.ipython_display(fps=15, loop=True, autoplay=True, maxduration=120)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X9SvyLia_hT"
      },
      "source": [
        "At this point, you know everything you need to know to create a behavioral map for a set of datasets. But this is where we can actually start doing some science!\n",
        "\n",
        "Open the project directory on the left pane. We should see some files in *Fly_Leap_mmpy/TSNE* or Fly_Leap_mmpy/UMAP folder. The 2-dimensional embeddings of all the files we've used for this project can be found in the **zVals_wShed_groups.mat** file which can be opened using hdf5storage in Python (as we did in the previous cell) or natively in MATLAB. This is where we can start doing some science!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVrrrOmu8Ywb"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.imshow(\n",
        "wshedfile['density'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOf3NnCq94SH"
      },
      "outputs": [],
      "source": [
        "wshedfile['zValNames']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDoZvt6Z-RYv"
      },
      "outputs": [],
      "source": [
        "wshedfile['zValLens']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nJgioFq9mdx"
      },
      "outputs": [],
      "source": [
        "wshedfile['watershedRegions'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKsrjks-KfN"
      },
      "outputs": [],
      "source": [
        "\n",
        "wshedfile['zValues'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j66bqndnevGe"
      },
      "outputs": [],
      "source": [
        "list(wshedfile.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWRss0HMefip"
      },
      "source": [
        "## 8.2 Create region videos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzuQ3TLgcIDJ"
      },
      "source": [
        "Now we'll try to create region videos - we'll pick contiguous time points that belong in one watershed region, and see what the animals is doing at those times by creating a movie.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFqSm9a8FZJr"
      },
      "outputs": [],
      "source": [
        "def makeGroupsAndSegments(watershedRegions, zValLens, min_length=10, max_length=100):\n",
        "\n",
        "  inds = np.zeros_like(watershedRegions)\n",
        "  start = 0\n",
        "  for l in zValLens:\n",
        "      inds[start:start + l] = np.arange(l)\n",
        "      start += l\n",
        "  vinds = np.digitize(np.arange(watershedRegions.shape[0]), bins=np.concatenate([[0], np.cumsum(zValLens)]))\n",
        "\n",
        "  splitinds = np.where(np.diff(watershedRegions, axis=0) != 0)[0] + 1\n",
        "  inds = [i for i in np.split(inds, splitinds) if len(i) > min_length and len(i) < max_length]\n",
        "  wregs = [i[0] for i in np.split(watershedRegions, splitinds) if len(i) > min_length and len(i) < max_length]\n",
        "\n",
        "  vinds = [i for i in np.split(vinds, splitinds) if len(i) > min_length and len(i) < max_length]\n",
        "  groups = [np.empty((0, 3), dtype=int)] * watershedRegions.max()\n",
        "\n",
        "  for wreg, tind, vind in zip(wregs, inds, vinds):\n",
        "      if np.all(vind == vind[0]):\n",
        "          groups[wreg - 1] = np.concatenate(\n",
        "              [groups[wreg - 1], np.array([vind[0], tind[0] + 1, tind[-1] + 1])[None, :]])\n",
        "  groups = np.array([[g] for g in groups], dtype=object)\n",
        "  return groups\n",
        "\n",
        "\n",
        "def makeregionvideo(region, parameters, wshedfile):\n",
        "\n",
        "\n",
        "  animfps=50.0\n",
        "  subs=2\n",
        "  submaxframes = 500\n",
        "\n",
        "\n",
        "  groups = makeGroupsAndSegments(wshedfile['watershedRegions'][0], wshedfile['zValLens'][0])\n",
        "  nregs = len(groups)\n",
        "\n",
        "  region = region-1\n",
        "\n",
        "  outputdir = f'{parameters.projectPath}/{parameters.method}/region_vidoes_{nregs}/'\n",
        "  if not os.path.exists(outputdir):\n",
        "    os.mkdir(outputdir)\n",
        "  groups = groups-1\n",
        "  print(f'[Region {region+1}] Starting')\n",
        "\n",
        "  if os.path.isfile(outputdir + 'regions_' + f'region+1:.3i'+ '.mp4'):\n",
        "      print(f'[Region {region+1}] Already present. ')\n",
        "      return\n",
        "\n",
        "  tqdm._instances.clear()\n",
        "\n",
        "  if not groups[region][0].shape[0] or groups[region][0].shape[0] == 1:\n",
        "      print(f'[Region {region+1}] No frames in groups.')\n",
        "      return\n",
        "\n",
        "  nframes = np.atleast_1d(np.diff(groups[region][0][:, 1:], axis=1).squeeze())\n",
        "  if np.sum(nframes < submaxframes) == 0:\n",
        "      print(f'[Region %i] All frames sequences more than length {submaxframes}.')\n",
        "      return\n",
        "\n",
        "  nplots = min(subs * subs, np.sum(nframes < submaxframes))\n",
        "  longinds = np.where(nframes < submaxframes)[0]\n",
        "  selectedclips = longinds[np.argsort(nframes[longinds])[::-1]][:nplots]\n",
        "\n",
        "  vidindslist = groups[region][0][selectedclips, 0]\n",
        "  framestoplot = np.array([np.arange(groups[region][0][i, 1], groups[region][0][i, 2]) for i in selectedclips], dtype=object)\n",
        "  maxsize = max([i.shape[0] for i in framestoplot])\n",
        "\n",
        "  print(f'[Region {region+1}] Making region video...')\n",
        "\n",
        "\n",
        "  subx = max(2, int(np.ceil(np.sqrt(nplots))))\n",
        "  fig, axes = plt.subplots(subx, subx, figsize=(12, 12))\n",
        "  fig.subplots_adjust(0, 0, 1.0, 1.0, 0.0, 0.0)\n",
        "\n",
        "  def make_frame(t):\n",
        "      j_ = int(t * animfps)\n",
        "      for i in range(subx * subx):\n",
        "\n",
        "          ax = axes[i // subx, i % subx]\n",
        "          ax.clear()\n",
        "          ax.axis('off')\n",
        "          if i >= nplots:\n",
        "              continue\n",
        "          j = j_ % len(framestoplot[i])\n",
        "          clip = clips[vidindslist[i]]\n",
        "          ax.imshow(clip.get_frame(framestoplot[i][j]/clip.fps),\n",
        "                    cmap='Greys_r', origin='lower')\n",
        "      return mplfig_to_npimage(fig)\n",
        "\n",
        "  try:\n",
        "      tqdm._instances.clear()\n",
        "  except:\n",
        "      pass\n",
        "\n",
        "  t1 = time.time()\n",
        "  animation = VideoClip(make_frame, duration=maxsize / animfps)\n",
        "\n",
        "  animation.write_videofile(outputdir + 'regions_' + f'{region+1}.3i' + '.mp4', fps=animfps, audio=False,\n",
        "                            threads=1)\n",
        "\n",
        "  print(f'[Region {region+1}] {time.time()-t1} seconds, Saved at {outputdir}regions_{region+1:.3i}.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8KvtmGD-xex"
      },
      "outputs": [],
      "source": [
        "makeregionvideo(10, parameters, wshedfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPAVaIKzY1m6"
      },
      "outputs": [],
      "source": [
        "# This creates region videos for all the region. This can take a while to run so be careful!\n",
        "wmax = wshedfile['watershedRegions'].max()\n",
        "print(wmax)\n",
        "for i in range(1, wmax+1):\n",
        "    makeregionvideo(i, parameters, wshedfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64hriJsbbPOE"
      },
      "outputs": [],
      "source": [
        "# Set region below to see your video.\n",
        "region = 10\n",
        "\n",
        "print(f'Region {region}')\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "outputdir = f\"{parameters.projectPath}/{parameters.method}/region_vidoes_{wshedfile['watershedRegions'].max()}/\"\n",
        "mp4 = open(f'{outputdir}/regions_{region:.3i}.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(f\"\"\"\n",
        "<video width=400 controls loop autoplay>\n",
        "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sHy6AF8YY25"
      },
      "source": [
        "\n",
        "\n",
        "You can also zip your project folder by calling\n",
        "```!zip -r Fly_Leap_mmpy.zip Fly_Leap_mmpy```\n",
        "and download the folder on your local computer to play around with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySony2raayOg"
      },
      "outputs": [],
      "source": [
        "!zip -r Fly_Leap_mmpy.zip Fly_Leap_mmpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl1TfHhQXAEE"
      },
      "source": [
        "# Transition Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_twbCv0yazsF"
      },
      "outputs": [],
      "source": [
        "wregs = wshedfile['watershedRegions'][0]\n",
        "wregs = np.split(wregs, np.cumsum(wshedfile['zValLens'][0])[:-1])\n",
        "transitions = [mmpy.demoutils.getTransitions(w[w>0]) for w in wregs]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-FfsTN-YkKY"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 4, figsize=(16,5))\n",
        "\n",
        "statevals = np.arange(1,wshedfile['watershedRegions'].max()+2)\n",
        "for ax, d in zip(axes.flatten(), [1, 10, 100, 1000]):\n",
        "    F = mmpy.demoutils.makeTransitionMatrix(np.concatenate(transitions), d)\n",
        "    ax.imshow(F, cmap='PuRd', extent=(statevals[0], statevals[-1], statevals[0], statevals[-1]))\n",
        "    ax.set_xlabel('Initial State')\n",
        "    ax.set_ylabel('Final State')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bRLtqCyYeZ_"
      },
      "outputs": [],
      "source": [
        "_ = mmpy.demoutils.plotLaggedEigenvalues(transitions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IExLdacW5Uf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
