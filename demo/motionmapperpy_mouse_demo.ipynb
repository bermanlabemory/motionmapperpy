{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-QgPR6PUnGmP"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bermanlabemory/motionmapperpy/blob/master/demo/motionmapperpy_mouse_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The github repositories for motionmapper are - \n",
        "\n",
        "1. [MATLAB] **motionmapper** : https://github.com/gordonberman/MotionMapper\n",
        "\n",
        "2. [PYTHON] **motionmapperpy** : https://github.com/bermanlabemory/motionmapperpy/\n"
      ],
      "metadata": {
        "id": "PbKKSrXoXFDv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihl-KLNKWPC-"
      },
      "source": [
        "# 1.&nbsp; Downloading and installing motionmapperpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CrqmGh9CC1g"
      },
      "source": [
        "First, we'll need to get motionmapperpy (sometimes we'll call it **mmpy** for brevity) from GitHub [![image.png](data:image/png;base64,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)](https://github.com/bermanlabemory/motionmapperpy). \n",
        "\n",
        "Below, we clone this github repository, which will download a copy of the repository on this COLAB runtime. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyYdRU07KSDB"
      },
      "source": [
        "!git clone https://github.com/bermanlabemory/motionmapperpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgBj7TeVDv6q"
      },
      "source": [
        "You'll notice the a **folder** named *motionmapperpy* created in the working directory in the left pane. This is the repository we just cloned and it contains the mmpy package and some toy datasets in the data folder. Note that our current working path is **/content/**, in case you ever get lost. We still need to install motionmapperpy as a python package and we can do that by running the commands in the cell below. \n",
        "\n",
        "*Quick note* : Colab instances come with many Python packages pre installed. You can run ```%pip list``` to see what packages are already present. \n",
        "\n",
        "*Another quick note* : We're installing a specific version of imageio below to avoid some dependency issues between ```moviepy``` and ```imageio``` that come preinstalled in colab. The up-to-date version packages seems to be behaving on my local machine so its likely colab.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYIdzKuADUu1",
        "collapsed": true
      },
      "source": [
        "## Install mmpy in this cell \n",
        "\n",
        "# Change to the motionmapperpy directory we just cloned into this colab instance\n",
        "%cd motionmapperpy\n",
        "\n",
        "# Install motionmapperpy as a python package to the current python environment\n",
        "!python setup.py install\n",
        "\n",
        "# Come out of the mmpy directory. \n",
        "%cd ..\n",
        "\n",
        "########################\n",
        "\n",
        "# Installing imageio 2.4.1 for resolving issues with moviepy.\n",
        "!pip3 install imageio==2.4.1\n",
        "\n",
        "\n",
        "# Installing ipympl for plotting interactive widgets in this notebook.\n",
        "!pip3 install ipympl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uySr8CInEfKJ"
      },
      "source": [
        "\\\n",
        "\\\n",
        "\\\n",
        "Great! We should have `motionmapperpy` installed on this colab runtime now! We'll **need to restart the runtime** so that this notebook is able to recognize motionmapperpy (meaning we can do ```import motionmapperpy```)\n",
        "and its dependencies as python packages. We can restart the instance by going to **Runtime->Restart Runtime** in the **top menu bar**. It is equivalent to restarting the ipython kernel when working with Jupyter notebooks. \n",
        "\n",
        "Note that restarting the runtime does not delete files and folders we have created in this colab instance, or remove any python packages we've installed here. But doing **Disconnect and delete runtime** will do all of those things so be careful! Also, note that Google will clear our colab instance if we're not using the instance for some arbitrarily brief amount, in which case the runtime will be factory reset and we'll lose .\n",
        "\n",
        "\n",
        "Once you have restarted the runtime, you are good to move on to the next section!\n",
        "your_text_here\n",
        "\n",
        "--------------------------------------------------------------------------\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "<font size=\"10\">**Restart runtime before proceeding below!**</font>\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "Great! Lets now import all packages we'll use in this notebook, including motionmapperpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HmksBlkYC7-"
      },
      "source": [
        "# Python standard library packages to do file/folder manipulations,\n",
        "# pickle is a package to store python variables\n",
        "import glob, os, pickle, sys\n",
        "\n",
        "# time grabs current clock time and copy to safely make copies of large \n",
        "# variables in memory.\n",
        "import time, copy \n",
        "\n",
        "# datetime package is used to get and manipulate date and time data\n",
        "from datetime import datetime\n",
        "\n",
        "# this packages helps load and save .mat files older than v7\n",
        "import hdf5storage \n",
        "\n",
        "# numpy works with arrays, pandas used to work with fancy numpy arrays\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# matplotlib is used to plot and animate to make movies\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "\n",
        "# moviepy helps open the video files in Python\n",
        "from moviepy.editor import VideoClip, VideoFileClip\n",
        "from moviepy.video.io.bindings import mplfig_to_npimage\n",
        "\n",
        "# Scikit-learn is a go-to library in Python for all things machine learning\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# tqdm helps create progress bars in for loops \n",
        "from tqdm import tqdm \n",
        "\n",
        "# Scipy is a go-to scientific computing library. We'll use it for median filtering. \n",
        "from scipy.ndimage import median_filter\n",
        "\n",
        "# Tensorflow and keras if we choose to do deep learning.\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as k\n",
        "\n",
        "# Configuring matplotlib to show animations in a colab notebook as javascript \n",
        "# objects for easier viewing. \n",
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "import ipywidgets\n",
        "\n",
        "%matplotlib widget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import motionmapperpy as mmpy\n",
        "\n",
        "# utility function used in this demo\n",
        "from motionmapperpy import demoutils"
      ],
      "metadata": {
        "id": "GMpiX82MBcvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feayV1ZNYDT5"
      },
      "source": [
        "# 2.&nbsp; Toy datasets "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLb1_VPTGTyT"
      },
      "source": [
        "There are two small datasets present within the motionmapper repository that we just cloned from GitHub. These are present in **`motionmapperpy/data`** path.\n",
        "\n",
        "1. **Mouse dataset** : **data/mice/** has 3 videos, each containing a mouseÂ moving freely inside an arena for 5 mins (taken at 30hz). The **.npy** files in that folder are numpy arrays containing 18 tracked keypoints on the mouse in the corresponding videos.\n",
        "\n",
        "3. **Leap tracked fly dataset** : This dataset has two movies **fly_leap_test.mp4** and **fly_leap_test_2.mp4** with 2 corresponding h5 files containing 32 points tracked using [LEAP](https://dataspace.princeton.edu/handle/88435/dsp01pz50gz79z).\n",
        "\n",
        "You can download the mp4 files on your local computer and check out what they look like!\n",
        "\n",
        "\n",
        "\n",
        "--------------------\n",
        "\n",
        "\n",
        "Alright, lets get started! We'll focus on the Mouse dataset as our toy example in this notebook. We'll import some packages to kick things off below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGMipKyinZYx"
      },
      "source": [
        "Now we can load the files associated with this dataset. We'll read the two video files using `moviepy` package, and the two .npy tracking datasets using numpy. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetnames = [d.split('/')[-1].split('.')[0] for d in glob.glob('motionmapperpy/data/mice/*.npy')]\n",
        "\n",
        "print(datasetnames)\n",
        "\n",
        "clips = [VideoFileClip('/content/motionmapperpy/data/mice/%s.mp4'%d) for d in datasetnames]\n",
        "h5s = [np.load('/content/motionmapperpy/data/mice/%s.npy'%d) for d in datasetnames]"
      ],
      "metadata": {
        "id": "V6QE6dhlMxEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvb0TsU-o9pj"
      },
      "source": [
        "Let's first explore some properties of the loaded movie clips and tracking data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVFvCalJpFEH"
      },
      "source": [
        "for i,(m,d) in enumerate(zip(clips, datasetnames)):\n",
        "  print('%s is %i seconds long at %i fps. '\n",
        "  'The frames are %i px wide and %i px high.'%(d, m.duration, m.fps, m.w, m.h))\n",
        "print()\n",
        "for i,h5 in enumerate(h5s):\n",
        "  print('.npy file %i has shape %s.'%(i, h5.shape))\n",
        "\n",
        "print('\\n\\nThere are %i tracked points on the mouse.'%h5s[0].shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets plot the x positions of all keypoints from the first dataset below."
      ],
      "metadata": {
        "id": "qmTzEkL-42r3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib widget\n",
        "fig, ax = plt.subplots(figsize=(14,5))\n",
        "_ = ax.plot(h5s[0][:1000,:,0]) #try 16th keypoint"
      ],
      "metadata": {
        "id": "znSkY15gdIhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvLpuxIa0GWY"
      },
      "source": [
        "Great! Now we are ready to plot and see what our dataset looks like. Below we'll use `matplotlib` to overlay tracking data on the video files. We'll read video frames using moviepy `videofileclip` objects stored in `clips`. Running this cell may take upto **1 minute**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the data\n"
      ],
      "metadata": {
        "id": "uCK8UnekBD-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "connections = [[0, 2, 5, 11, 13, 14, 15 ,16], [0, 1, 4, 7, 10, 13], [0, 3, 6, 9, 12, 13], [1, 2, 3], [4, 5, 6], [7, 8, 9],\n",
        "                   [10, 11, 12]]\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "\n",
        "fig.subplots_adjust(0,0,1,1,0,0)\n",
        "\n",
        "def makeframe(dname, frameno):\n",
        "    ax.clear()\n",
        "    print('Making ', frameno,end='\\r')\n",
        "    frameno = int(min(frameno, clips[dname].duration*clips[dname].fps-1))\n",
        "    ax.imshow(clips[dname].get_frame(frameno/clips[dname].fps))\n",
        "    for conn in connections:\n",
        "        ax.plot(h5s[dname][frameno,conn,0], h5s[dname][frameno,conn,1], color='firebrick')\n",
        "    ax.axis('off')\n",
        "    ax.set_title('%s, Frame %i'%(datasetnames[dname], frameno))\n",
        "    print('',end='\\r')\n",
        "    \n",
        "    \n",
        "\n",
        "w = ipywidgets.FloatSlider\n",
        "dd = ipywidgets.Dropdown\n",
        "sliders = {'dname':dd(options=[(d,i) for i,d in enumerate(datasetnames)], index=0,layout=ipywidgets.Layout(width='1000px')),\n",
        "        'frameno':w(value=0.0, min=0, max=len(h5s[0]), step=1, continuous_update=True, orientation='horizontal', \n",
        "                    description='Frame No.',layout=ipywidgets.Layout(width='1000px'))}\n",
        "\n",
        "i = ipywidgets.interactive_output(makeframe, sliders)\n",
        "\n",
        "b = ipywidgets.VBox([sliders['dname'], sliders['frameno']])\n",
        "\n",
        "display(b, i)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DSup4mn36JYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.&nbsp; Pre-processing the data\n",
        "\n",
        "[![](http://www.accutrend.com/wp-content/uploads/2018/07/GiGo-570x315.jpg)](http://www.accutrend.com/it-still-comes-down-to-garbage-in-garbage-out/)\n",
        "\n",
        "When working with any model, algorithm or pipeline, we have to be mindful of the data we are feeding into them. This section covers one (of many) ways to process input data before feeding into motionmapperpy. \n",
        "I packed a few utility functions for this demo in the motionmapperpy repository submodule called ```demoutils``` (we imported it already) and we'll use them here and there."
      ],
      "metadata": {
        "id": "QIrjizpjQ3vj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Egocentering"
      ],
      "metadata": {
        "id": "b3FSYz-hB2re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Now we'll use a handy function to 'egocenter' our postural time series, such that the pose is centered at origin and is always pointing to the right. We can use the ```egoh5``` function to do that. "
      ],
      "metadata": {
        "id": "K5N7UnJE79aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "egoh5s = [demoutils.egoh5(h5) for h5 in h5s]"
      ],
      "metadata": {
        "id": "NPhmeAzGAOjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check what that did to our data."
      ],
      "metadata": {
        "id": "1MFprbIE8fsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib widget\n",
        "connections2 = [[0, 3, 6, 8, 11, 12], [0, 2, 5, 10, 12], [0, 1, 4, 7, 9, 12], \n",
        "                [1, 2, 3], [4, 5, 6], [7, 8], [9, 10, 11], [12,13,14,15]]\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "\n",
        "fig.subplots_adjust(0,0,1,1,0,0)\n",
        "\n",
        "def makeframe(dname, frameno):\n",
        "    ax.clear()\n",
        "    print('Making ', frameno,end='\\r')\n",
        "    frameno = int(min(frameno, clips[dname].duration*clips[dname].fps-1))\n",
        "    for conn in connections2:\n",
        "        ax.plot(egoh5s[dname][frameno,conn,0], egoh5s[dname][frameno,conn,1], color='firebrick')\n",
        "    # ax.axis('off')\n",
        "    ax.set_title('%s, Frame %i'%(datasetnames[dname], frameno))\n",
        "    ax.set_xlim([-200,150])\n",
        "    ax.set_ylim([-200,150])\n",
        "    ax.set_aspect('equal')\n",
        "    print('',end='\\r')\n",
        "    \n",
        "    \n",
        "\n",
        "w = ipywidgets.FloatSlider\n",
        "dd = ipywidgets.Dropdown\n",
        "sliders = {'dname':dd(options=[(d,i) for i,d in enumerate(datasetnames)], index=0,layout=ipywidgets.Layout(width='1000px')),\n",
        "        'frameno':w(value=0.0, min=0, max=len(h5s[0]), step=1, continuous_update=True, orientation='horizontal', \n",
        "                    description='Frame No.',layout=ipywidgets.Layout(width='1000px'))}\n",
        "\n",
        "i = ipywidgets.interactive_output(makeframe, sliders)\n",
        "\n",
        "b = ipywidgets.VBox([sliders['dname'], sliders['frameno']])\n",
        "\n",
        "display(b, i)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1Z4LfK3T8jNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rescaling\n",
        "Great! Now, since we're measuring postural dynamics from multiple \n",
        "animals, varying body sizes across individuals can contribue a lot of noisy dynamics and mess up our quantification. So, lets first check the median nose-to-tail length of each of the mice and then normalize them"
      ],
      "metadata": {
        "id": "nLQJAz-e9qdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8, 4))\n",
        "spec = fig.add_gridspec(ncols=7, nrows=2)\n",
        "\n",
        "ax = fig.add_subplot(spec[0,:])\n",
        "ax.boxplot([np.linalg.norm(eh5[:,14]-eh5[:,0], axis=1) for eh5 in egoh5s])\n",
        "ax.set_xlabel('Mice #')\n",
        "ax.set_ylabel('Nose to Tail base (px)')\n",
        "\n",
        "ax = fig.add_subplot(spec[1, :-1])\n",
        "ax.boxplot([np.linalg.norm(eh5[:,14]-eh5[:,0], axis=1) for eh5 in egoh5s])\n",
        "ax.set_xlabel('Mice #')\n",
        "ax.set_ylabel('Nose to Tail base (px)')\n",
        "ax.set_ylim([150,250])\n",
        "\n",
        "ax = fig.add_subplot(spec[1,-1])\n",
        "ax.hist([np.median(np.linalg.norm(eh5[:,14]-eh5[:,0], axis=1)) for eh5 in egoh5s], orientation='horizontal', bins=20, fc=\"None\", edgecolor='black')\n",
        "ax.set_ylim([150,250])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3T-bD5Yns3kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize postural timeseries across individuals\n",
        "\n",
        "mice_lengths = np.array([np.median(np.linalg.norm(eh5[:,14]-eh5[:,0], axis=1)) for eh5 in egoh5s])\n",
        "mice_lengths_normed = mice_lengths/np.max(mice_lengths)\n",
        "assert np.all(np.isfinite(mice_lengths_normed))\n",
        "\n",
        "egoh5s_noconf_scaled = [eh5/scale for eh5,scale in zip(egoh5s, mice_lengths_normed)]\n",
        "alleh5s = np.concatenate(egoh5s_noconf_scaled, axis=0)"
      ],
      "metadata": {
        "id": "ohraaAAhtNp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoEncoder Model"
      ],
      "metadata": {
        "id": "wVsYy463-cWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll train an AutoEncoder model to reduce the dimensionality of the data (we hav 32 dimensions right now, 16 keypoints in 2D). \n",
        "\n",
        "Lets create the train and validation datasets for training the model. "
      ],
      "metadata": {
        "id": "PgkoQulGCDma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_alleh5s = alleh5s[np.where(~np.any(np.any(np.abs(alleh5s)>200, axis=2), axis=1))[0]]\n",
        "\n",
        "trainx = clean_alleh5s[5::5, :].reshape((-1, 32)).astype('float32')[:,:]\n",
        "valx = clean_alleh5s[7::19, :].reshape((-1, 32)).astype('float32')[:,:]\n",
        "print(trainx.shape, valx.shape)\n",
        "print(trainx.min(), trainx.max())\n",
        "print(valx.min(), valx.max())"
      ],
      "metadata": {
        "id": "kCIdpkx5tNcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets now define and train the model!"
      ],
      "metadata": {
        "id": "KYfdfGnNDQW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ident = 'RescaledMice_8latents'"
      ],
      "metadata": {
        "id": "_oaHa6p2Db46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "ANIOuWLLESVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AE = tf.keras.Sequential([tf.keras.layers.InputLayer(input_shape=(32,)),\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dense(8, activation=None),\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dense(32, activation=None)])\n",
        "AE.compile('adam', 'mse')\n",
        "trainhist = AE.fit(trainx, trainx, batch_size=64, epochs=200, validation_data=(valx, valx), \n",
        "              callbacks = [tf.keras.callbacks.ModelCheckpoint('%s.h5'%ident, \n",
        "                                                              monitor=\"val_loss\", \n",
        "                                                              verbose=1, \n",
        "                                                              save_best_only=True)])\n",
        "pickle.dump(trainhist, open('%s.hist'%ident, 'wb'))\n",
        "AE_best = k.models.load_model('%s.h5'%ident)"
      ],
      "metadata": {
        "id": "OvrrotVftNMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at the loss curves below. "
      ],
      "metadata": {
        "id": "9CC1rNwyD9Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10,4))\n",
        "ax.plot(trainhist.history['loss'], label='Training Loss')\n",
        "ax.plot(trainhist.history['val_loss'], label='Validation Loss')\n",
        "ax.set_yscale('log')\n",
        "ax.set_ylabel('Mean Squared Error (px^2)')\n",
        "ax.set_xlabel('Epochs #')\n",
        "ax.legend()"
      ],
      "metadata": {
        "id": "TEDhm146EH-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Validation Errors\n",
        "\n",
        "We can also look at the predictions on the validation data, and plot the predictions with highest errors."
      ],
      "metadata": {
        "id": "UTuvNizAEt-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get validation set predictions\n",
        "\n",
        "valx_pred = AE_best.predict(valx, verbose=1)\n",
        "assert np.all(np.mean(np.abs(valx-valx_pred), axis=1)>0)"
      ],
      "metadata": {
        "id": "5OneRIgDunt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "connections2 = [[0, 3, 6, 8, 11, 12], [0, 2, 5, 10, 12,13,14,15], [0, 1, 4, 7, 9, 12], \n",
        "                [1, 2, 3], [4, 5, 6], [7, 8], [9, 10, 11], [12,13,14,15]]\n",
        "\n",
        "badvalinds = np.argsort(np.mean(np.abs(valx-valx_pred), axis=1))[::-1]\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "\n",
        "# fig.subplots_adjust(0,0,1,1,0,0)\n",
        "\n",
        "def makeframe(tin):\n",
        "    skip=1\n",
        "    ax.clear()\n",
        "    lines = []\n",
        "    t = badvalinds[tin]\n",
        "    # print(t, tin)\n",
        "    for conn in connections2[:-1]:\n",
        "        conn = [2*i for i in conn]\n",
        "        lines.append(ax.plot(valx[t*skip, conn], valx[t*skip, [i+1 for i in conn]], color='royalblue')[0])\n",
        "        lines.append(ax.plot(valx_pred[t*skip, conn], valx_pred[t*skip, [i+1 for i in conn]], color='firebrick')[0])\n",
        "    ax.set_title('(%i, %i)'%((tin, t)))\n",
        "#     ax.set_aspect('equal')\n",
        "    ax.set_xlim([-200, 100])\n",
        "    ax.set_ylim([-120,120])\n",
        "    \n",
        "    \n",
        "\n",
        "i = ipywidgets.IntSlider\n",
        "sliders = {'tin':i(value=0, min=0, max=50, step=1, #len(badvalinds)-1\n",
        "                   continuous_update=False, orientation='horizontal', \n",
        "                    description='Frame No.',layout=ipywidgets.Layout(width='1000px'))}\n",
        "\n",
        "i = ipywidgets.interactive_output(makeframe, sliders)\n",
        "\n",
        "b = ipywidgets.VBox([sliders['tin']])\n",
        "\n",
        "display(b, i)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ajImwqcmFOkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can quantify mean errors in pixels along the x and y dimensions below. "
      ],
      "metadata": {
        "id": "wmhufy7OGPLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "errors = np.abs(valx_pred-valx).reshape((-1, valx.shape[1]//2, 2))\n",
        "np.mean(errors, axis=(0,1))"
      ],
      "metadata": {
        "id": "890uM4q2xPuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the 8-dimensional bottleneck layer values as the input for motionmapperpy. We get these values below. "
      ],
      "metadata": {
        "id": "5NxUftpsxVNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AE2 = k.models.Model(inputs=AE_best.input, outputs=AE_best.layers[1].output)\n",
        "\n",
        "#check if the weights are same \n",
        "assert np.all([np.all(w1==w2) for w1,w2 in zip(AE_best.get_weights(),AE2.get_weights())])\n",
        "\n",
        "print(alleh5s.shape)\n",
        "\n",
        "alleh5s_ae = AE2.predict(alleh5s[:,:].reshape((-1, 32)).astype('float32'), verbose=1)\n",
        "print(alleh5s_ae.shape)"
      ],
      "metadata": {
        "id": "DdjoBi77xPoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhuNfeVm4T1P"
      },
      "source": [
        "# 4.&nbsp; Creating an mmpy project directory\n",
        "\n",
        "Now that we have a low dimensional time series which **may** not set Google servers on fire, we will create our project directory for running the `motionmapperpy` pipeline on the data we have. Having a project directory is awesome, as it helps us stay organized when working with big datasets and multiple files. It allows datasets to be easily referenced and loaded without exhausting memory, and we can store pipeline outputs in well-defined and easy to read files.\n",
        "\n",
        "\n",
        "Lets start by importing `motionmapperpy` and creating a project directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smnR7O9Y4idG"
      },
      "source": [
        "import motionmapperpy as mmpy\n",
        "\n",
        "\n",
        "projectPath = '/content/Mice_mmpy'\n",
        "\n",
        "# This creates a project directory structure which will be used to store all motionmappery pipeline\n",
        "# related data in one place.\n",
        "\n",
        "mmpy.createProjectDirectory(projectPath)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_uosHZN5mqS"
      },
      "source": [
        "Now lets store the two low-d time series to the *`Projections`* folder that got created in the cell above.\n",
        "\n",
        "Note the _pcaModes.mat identifier when saving these files.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDOKMsD15_l-"
      },
      "source": [
        "projs_list = np.split(alleh5s_ae, np.cumsum([len(e) for e in egoh5s])[:-1])\n",
        "\n",
        "for i,projs in enumerate(projs_list):\n",
        "    hdf5storage.savemat('%s/Projections/%s_pcaModes.mat'%(projectPath, datasetnames[i]), {'projections':projs})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocFZeYek6gO_"
      },
      "source": [
        "We'll now go through `mmpy` parameters. They are a handful and can be overwhelming, but they are very interpretable! \n",
        "\n",
        "Parameters are cruicial to `mmpy` as they lay out some hard-coded choices *we need to make* when running this pipeline. I will explain each parameter as we encounter them in the cell below, so please read through this cell below as you run it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_QuCClWcDHm"
      },
      "source": [
        "\"\"\"2. Setup run parameters for MotionMapper.\"\"\"\n",
        "\n",
        "#% Load the default parameters.\n",
        "parameters = mmpy.setRunParameters() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# %%%%%%% PARAMETERS TO CHANGE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "# These need to be revised everytime you are working with a new dataset. #\n",
        "\n",
        "parameters.projectPath = projectPath #% Full path to the project directory.\n",
        "\n",
        "\n",
        "parameters.method = 'UMAP' #% We can choose between 'TSNE' or 'UMAP'\n",
        "\n",
        "parameters.minF = 1        #% Minimum frequency for Morlet Wavelet Transform\n",
        "\n",
        "parameters.maxF = 15       #% Maximum frequency for Morlet Wavelet Transform,\n",
        "                           #% usually equals to the Nyquist frequency for your\n",
        "                           #% measurements.\n",
        "\n",
        "parameters.samplingFreq = 30    #% Sampling frequency (or FPS) of data.\n",
        "\n",
        "parameters.numPeriods = 30       #% No. of dyadically spaced frequencies to\n",
        "                                 #% calculate between minF and maxF.\n",
        "\n",
        "parameters.pcaModes = 8 #% Number of low-d features.\n",
        "\n",
        "parameters.numProcessors = -1     #% No. of processor to use when parallel\n",
        "                                 #% processing for wavelet calculation (if not using GPU)  \n",
        "                                 #% and for re-embedding. -1 to use all cores \n",
        "                                 #% available.\n",
        "\n",
        "parameters.useGPU = -1           #% GPU to use for wavelet calculation, \n",
        "                                 #% set to -1 if GPU not present.\n",
        "\n",
        "parameters.training_numPoints = 3000    #% Number of points in mini-trainings.\n",
        "\n",
        "\n",
        "# %%%%% NO NEED TO CHANGE THESE UNLESS MEMORY ERRORS OCCUR %%%%%%%%%%\n",
        "\n",
        "parameters.trainingSetSize = 5000  #% Total number of training set points to find. \n",
        "                                 #% Increase or decrease based on\n",
        "                                 #% available RAM. For reference, 36k is a \n",
        "                                 #% good number with 64GB RAM.\n",
        "\n",
        "parameters.embedding_batchSize = 30000  #% Lower this if you get a memory error when \n",
        "                                        #% re-embedding points on a learned map.\n",
        "\n",
        "# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
      ],
      "metadata": {
        "id": "5866VHTOlTL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU8RN67i_fZe"
      },
      "source": [
        "Above covers usually relevant parameters when using `mmpy`. However, there are parameters associated with tSNE and UMAP implementations, such as below, which aren't usually required to be changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3RBAwEg_75_"
      },
      "source": [
        "# %%%%%%% tSNE parameters %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "#% can be 'barnes_hut' or 'exact'. We'll use barnes_hut for this tutorial for speed.\n",
        "parameters.tSNE_method = 'barnes_hut' \n",
        "\n",
        "# %2^H (H is the transition entropy)\n",
        "parameters.perplexity = 32\n",
        "\n",
        "# %number of neigbors to use when re-embedding\n",
        "parameters.maxNeighbors = 200\n",
        "\n",
        "# %local neighborhood definition in training set creation\n",
        "parameters.kdNeighbors = 5\n",
        "\n",
        "# %t-SNE training set perplexity\n",
        "parameters.training_perplexity = 20\n",
        "\n",
        "\n",
        "# %%%%%%%% UMAP Parameters %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
        "\n",
        "# Size of local neighborhood for UMAP.\n",
        "n_neighbors = 15\n",
        "\n",
        "# Negative sample rate while training.\n",
        "train_negative_sample_rate = 5\n",
        "\n",
        "# Negative sample rate while embedding new data.\n",
        "embed_negative_sample_rate = 1\n",
        "\n",
        "# Minimum distance between neighbors.\n",
        "min_dist = 0.1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTh2DM5L6e8E"
      },
      "source": [
        "## Visualizing wavelet amplitudes\n",
        "\n",
        "This section is not required to be run by motionmapperpy, but we'll go through it to visualize spectrograms on one of the low-dimensional time series.\n",
        "\n",
        "We'll use `mmpy.findWavelets` function to obtain the waveletes, and plot the obtained spectrogram for each feature/projection. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lysbx4_k61KW"
      },
      "source": [
        "wlets, freqs = mmpy.findWavelets(projs_list[0], projs_list[0].shape[1], \n",
        "                                 parameters.omega0, parameters.numPeriods, \n",
        "                                 parameters.samplingFreq, parameters.maxF, \n",
        "                                 parameters.minF, parameters.numProcessors, \n",
        "                                 parameters.useGPU)\n",
        "\n",
        "fig, axes = plt.subplots(alleh5s_ae.shape[1], 1, figsize=(15,10))\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "  ax.imshow(wlets[:300,parameters.numPeriods*i:parameters.numPeriods*(i+1)].T, cmap='PuRd', origin='lower')\n",
        "  ax.set_yticks(np.arange(parameters.numPeriods, step=4))\n",
        "  ax.set_yticklabels(['%0.1f'%freqs[j] for j in np.arange(parameters.numPeriods, step=4)])\n",
        "  if i == 3:\n",
        "    ax.set_ylabel(\"Frequencies (hz)\", fontsize=14)\n",
        "  ax.set_title('Projection #%i'%(i+1))\n",
        "ax.set_xlabel('Frames', fontsize=14)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWZtyxgLBWyu"
      },
      "source": [
        "As we can see, our low-d time series is soon dwarfed by the 25-dimensional wavelet amplitudes obtained for each low-d feature! This is why its wise to spend some time reducing the dimensionality of our original data, as much as we can. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aNoAnng7Cla"
      },
      "source": [
        "# 5.&nbsp; Creating a training set and embedding it using tSNE/UMAP "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9eJ1AzK7ybU"
      },
      "source": [
        "Even though we are working with toy datasets, we have two extremely high dimensional timeseries we're using to create a smaller and more interpretable representation. tSNE and UMAP both need to compute all-to-all distances in high-dimensional space to find neighboring points and embed them closely on this low-dimensional space we're building. This computation can quickly exhaust memory (ram) and it scales quadratically with # of datapoints. \n",
        "\n",
        "To navigate this challenge, we do a subsampling procedure to create a training set, and use tSNE or UMAP to create training embeddings. All of this is done in the cell below.\n",
        "\n",
        "**Time taken** : TSNE ~2 mins | UMAP ~1 min\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRid9ecM7q3A"
      },
      "source": [
        "t1 = time.time()\n",
        "\n",
        "mmpy.subsampled_tsne_from_projections(parameters, parameters.projectPath)\n",
        "\n",
        "print('Done in %i seconds.'%(time.time()-t1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7QH0-yOEkgZ"
      },
      "source": [
        "Note that the `training set` and `training embedding` are both save in the `project_directory/TSNE` or `project_directory/UMAP` directories depending on which method you're using. We'll load the training embedding below and plot it. You can play around with the sigma value here to change the coarseness of the density map. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing training set embeddings."
      ],
      "metadata": {
        "id": "KceQBdVzKsqZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXfWBg7pAx2w"
      },
      "source": [
        "%matplotlib widget\n",
        "\n",
        "trainy = hdf5storage.loadmat('%s/%s/training_embedding.mat'%(parameters.projectPath, parameters.method))['trainingEmbedding']\n",
        "m = np.abs(trainy).max()\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12,6))\n",
        "axes[0].scatter(trainy[:,0], trainy[:,1], marker='.', c=np.arange(trainy.shape[0]), s=1)\n",
        "axes[0].set_xlim([-m-20, m+20])\n",
        "axes[0].set_ylim([-m-20, m+20])\n",
        "axes[0].set_title(trainy.shape)\n",
        "\n",
        "\n",
        "cbar_ax = fig.add_axes([0.92, 0.3, 0.01, 0.4])\n",
        "\n",
        "\n",
        "def makeframe(sigma, c_limit):\n",
        "    ax = axes[1]\n",
        "    ax.clear()\n",
        "    _, xx, density = mmpy.findPointDensity(trainy, sigma, 511, [-m-20, m+20])\n",
        "    im = ax.imshow(density, cmap=mmpy.gencmap(), extent=(xx[0], xx[-1], xx[0], xx[-1]), \n",
        "              origin='lower', vmax=np.max(density)*c_limit)\n",
        "    # ax.axis('off')\n",
        "    ax.set_title('%0.02f, %0.02f'%(sigma, c_limit))\n",
        "    cbar_ax.clear()\n",
        "    fig.colorbar(im, cax=cbar_ax, fraction=c_limit)\n",
        "\n",
        "\n",
        "w = ipywidgets.FloatSlider\n",
        "sliders = {'sigma':w(value=1.0, min=0.1, max=3.0, step=0.05, continuous_update=False, \n",
        "                     orientation='horizontal', \n",
        "                     description='sigma', layout=ipywidgets.Layout(width='1000px')),\n",
        "        'c_limit':w(value=0.95, min=0.1, max=1.0, step=0.05, continuous_update=False, \n",
        "                    orientation='horizontal', \n",
        "                    description='C Limit',layout=ipywidgets.Layout(width='1000px'))}\n",
        "\n",
        "i = ipywidgets.interactive_output(makeframe, sliders)\n",
        "\n",
        "b = ipywidgets.VBox([sliders['sigma'], sliders['c_limit']])\n",
        "\n",
        "display(b, i)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the left, we see a scatter plot and on the right, we see a Gaussian kernel convolved density estimation of these points. Does it surprise you? What does changing the sigma value do?  \n"
      ],
      "metadata": {
        "id": "h-B_oiewMXAC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW5jndTkW1of"
      },
      "source": [
        "# 6.&nbsp; Finding embeddings for all data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdCW8Y309RXu"
      },
      "source": [
        "Now, we can find embeddings for our entire dataset! We'll use the `mmpy.findEmbeddings` function which requires the training set and the 2-d embeddings we find in the last step, and the high-d 'projections' time series for each dataset. We'll save the obtained embeddings for each dataset neatly in the Projections folder so that we can reference them later.\n",
        "\n",
        "**Running time** : TSNE ~12 mins | UMAP ~1 min"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB84yPqS8u89"
      },
      "source": [
        "tall = time.time()\n",
        "\n",
        "import h5py\n",
        "tfolder = parameters.projectPath+'/%s/'%parameters.method\n",
        "\n",
        "# Loading training data\n",
        "with h5py.File(tfolder + 'training_data.mat', 'r') as hfile:\n",
        "    trainingSetData = hfile['trainingSetData'][:].T\n",
        "\n",
        "# Loading training embedding\n",
        "with h5py.File(tfolder+ 'training_embedding.mat', 'r') as hfile:\n",
        "    trainingEmbedding= hfile['trainingEmbedding'][:].T\n",
        "\n",
        "if parameters.method == 'TSNE':\n",
        "    zValstr = 'zVals' \n",
        "else:\n",
        "    zValstr = 'uVals'\n",
        "\n",
        "projectionFiles = glob.glob(parameters.projectPath+'/Projections/*pcaModes.mat')\n",
        "for i in range(len(projectionFiles)):\n",
        "    print('Finding Embeddings')\n",
        "    t1 = time.time()\n",
        "    print('%i/%i : %s'%(i+1,len(projectionFiles), projectionFiles[i]))\n",
        "\n",
        "\n",
        "    # Skip if embeddings already found.\n",
        "    if os.path.exists(projectionFiles[i][:-4] +'_%s.mat'%(zValstr)):\n",
        "        print('Already done. Skipping.\\n')\n",
        "        continue\n",
        "\n",
        "    # load projections for a dataset\n",
        "    projections = hdf5storage.loadmat(projectionFiles[i])['projections']\n",
        "\n",
        "    # Find Embeddings\n",
        "    zValues, outputStatistics = mmpy.findEmbeddings(projections,trainingSetData,trainingEmbedding,parameters)\n",
        "\n",
        "    # Save embeddings\n",
        "    hdf5storage.write(data = {'zValues':zValues}, path = '/', truncate_existing = True,\n",
        "                    filename = projectionFiles[i][:-4]+'_%s.mat'%(zValstr), store_python_metadata = False,\n",
        "                      matlab_compatible = True)\n",
        "    \n",
        "    # Save output statistics\n",
        "    with open(projectionFiles[i][:-4] + '_%s_outputStatistics.pkl'%(zValstr), 'wb') as hfile:\n",
        "        pickle.dump(outputStatistics, hfile)\n",
        "\n",
        "    del zValues,projections,outputStatistics\n",
        "\n",
        "print('All Embeddings Saved in %i seconds!'%(time.time()-tall))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing all embeddings"
      ],
      "metadata": {
        "id": "DQwUeRBLKynM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlhFOt0-G_cW"
      },
      "source": [
        "We can visualize the obtained embeddings by calling the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMHW7yDeJIy_"
      },
      "source": [
        "# load all the embeddings\n",
        "%matplotlib widget\n",
        "for i in glob.glob(parameters.projectPath+'/Projections/*_%s.mat'%(zValstr)):\n",
        "  ally = hdf5storage.loadmat(i)['zValues']\n",
        "\n",
        "m = np.abs(ally).max()\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12,6))\n",
        "axes[0].scatter(ally[:,0], ally[:,1], marker='.', c=np.arange(ally.shape[0]), s=1)\n",
        "axes[0].set_xlim([-m-20, m+20])\n",
        "axes[0].set_ylim([-m-20, m+20])\n",
        "axes[0].set_title(ally.shape)\n",
        "\n",
        "cbar_ax = fig.add_axes([0.92, 0.3, 0.01, 0.4])\n",
        "\n",
        "\n",
        "def makeframe(sigma, c_limit):\n",
        "    ax = axes[1]\n",
        "    ax.clear()\n",
        "    _, xx, density = mmpy.findPointDensity(ally, sigma, 511, [-m-20, m+20])\n",
        "    im = ax.imshow(density, cmap=mmpy.gencmap(), extent=(xx[0], xx[-1], xx[0], xx[-1]), \n",
        "              origin='lower', vmax=np.max(density)*c_limit)\n",
        "    # ax.axis('off')\n",
        "    ax.set_title('%0.02f, %0.02f'%(sigma, c_limit))\n",
        "    cbar_ax.clear()\n",
        "    fig.colorbar(im, cax=cbar_ax, fraction=c_limit)\n",
        "\n",
        "\n",
        "w = ipywidgets.FloatSlider\n",
        "sliders = {'sigma':w(value=1.0, min=0.1, max=3.0, step=0.05, continuous_update=False, \n",
        "                     orientation='horizontal', \n",
        "                     description='sigma', layout=ipywidgets.Layout(width='1000px')),\n",
        "        'c_limit':w(value=0.95, min=0.1, max=1.0, step=0.05, continuous_update=False, \n",
        "                    orientation='horizontal', \n",
        "                    description='C Limit',layout=ipywidgets.Layout(width='1000px'))}\n",
        "\n",
        "i = ipywidgets.interactive_output(makeframe, sliders)\n",
        "\n",
        "b = ipywidgets.VBox([sliders['sigma'], sliders['c_limit']])\n",
        "\n",
        "display(b, i)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.&nbsp; Watershed transform on the density map"
      ],
      "metadata": {
        "id": "hLoOcHS0OKbr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPqF-zDYANq"
      },
      "source": [
        "There is another handy function in `motionmapperpy` called `findWatershedRegions`. This will do an iterative watershed transform on the behavioral density map **until the given # of `minimum_regions` are found** in the density map.\n",
        "\n",
        "It saves watershed transformed output of the embedding in `project_director/UMAP/zVals_wShed_groups.mat` file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50zDsiEPLw4s"
      },
      "source": [
        "# decrease the startsigma values by 0.5 at a time if you get errors.\n",
        "startsigma = 3 if parameters.method == 'TSNE' else 1.0\n",
        "\n",
        "[os.remove(i) for i in glob.glob('%s/%s/zWshed*.png'%(parameters.projectPath, parameters.method))]\n",
        "\n",
        "mmpy.findWatershedRegions(parameters, minimum_regions=20, startsigma=startsigma, pThreshold=[0.33, 0.67],\n",
        "                     saveplot=True, endident = '*_pcaModes.mat')\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(glob.glob('%s/%s/zWshed*.png'%(parameters.projectPath, parameters.method))[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## zVals_wShed_groups.mat file\n",
        "\n",
        "The **UMAP/zVals_wShed_groups.mat** or **TSNE/zVals_wShed_groups.mat** contains all important outputs from the motionmapperpy pipeline. Lets look at some of the important variables in it."
      ],
      "metadata": {
        "id": "QFiBQnpbKmDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wshedfile = hdf5storage.loadmat('%s/%s/zVals_wShed_groups.mat'%(parameters.projectPath, parameters.method))\n",
        "print('Loaded %s/%s/zVals_wShed_groups.mat'%(parameters.projectPath, parameters.method))\n",
        "\n",
        "print(type(wshedfile))\n",
        "print(list(wshedfile.keys()))"
      ],
      "metadata": {
        "id": "YYMngVUaKlyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('wshedfile[\\'zValues\\'] contains the 2-d tsne/umap coordinates for all the datasets, concatenated.', \n",
        "      wshedfile['zValues'].shape)\n",
        "print('\\nwshedfile[\\'watershedRegions\\'] contains the watershed region assignment for all the datasets, concatenated.', \n",
        "      wshedfile['watershedRegions'].shape)\n",
        "print('\\nwshedfile[\\'zValLens\\'] contains the length of all datasets : ', \n",
        "      wshedfile['zValLens'], '\\n in the order they are in wshedfile[\\'zValNames\\'] : ', [z[0][0] for z in wshedfile['zValNames'][0]], end='\\n\\n')\n",
        "print('We can use these two to cross reference zValues and watersherRegions values to the original datasets.')"
      ],
      "metadata": {
        "id": "5B0gP1PuKlbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.&nbsp; Stereotypy in Behavior"
      ],
      "metadata": {
        "id": "RDAi4iO99DeG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is very important to know when are we capturing a stereotypic behavioral bout. There are two criteria we can use to decied that - \n",
        "\n",
        "**1. Velocity in the 2-d space**\n",
        "\n",
        "If we look at the velocity of trajectory within the 2-dimensional embeddings, we see a bimodal distribution, in which the lower velocity peak corresponds to local movement and the higher velocity peak corresponding to jumping back and forth between far away peaks. \n",
        "\n",
        "Lets create and see this distribution below. \n",
        "\n",
        "**Note : This only work with TSNE and not UMAP. Can we guess why?**"
      ],
      "metadata": {
        "id": "qvmGJ1Mc9IwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wshedfile = hdf5storage.loadmat('%s/%s/zVals_wShed_groups.mat'%(parameters.projectPath, parameters.method))\n",
        "ampV = wshedfile['v']\n",
        "ampVels = ampV * parameters['samplingFreq']\n",
        "vellog10all = np.log10(ampVels[ampVels > 0])\n",
        "npoints = min(50000, len(vellog10all))\n",
        "\n",
        "vellog10 = np.random.choice(vellog10all, size=npoints, replace=False)\n",
        "\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gm = GaussianMixture(n_components=2, verbose=1, tol=1e-5, max_iter=2000, n_init=1, reg_covar=1e-3)\n",
        "inds = np.random.randint(0, vellog10.shape[0], size=npoints)\n",
        "gm = gm.fit(vellog10[inds, None])\n",
        "minind = np.argmin(gm.means_.squeeze())\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "bins = ax.hist(vellog10, bins=200, density=True, color='k', alpha=0.5)\n",
        "bins = bins[1]\n",
        "p_score = np.exp(gm.score_samples(bins[:, None]))\n",
        "ax.plot(bins, p_score, color='k', alpha=0.5)\n",
        "\n",
        "for (c, compno, mu, sigma, p) in \\\n",
        "        zip(['royalblue', 'firebrick'], [1, 2], gm.means_.squeeze(), np.sqrt(gm.covariances_.squeeze()),\n",
        "            gm.weights_):\n",
        "    ax.plot(bins, mmpy.mmutils.getPDF(bins, mu, sigma, p), label='Component %i' % compno, color=c, alpha=0.5)\n",
        "\n",
        "ax.plot(bins, gm.predict_proba(bins[:, None])[:, minind], label='pRest')\n",
        "ax.axvline(bins[np.where(gm.predict_proba(bins[:, None])[:, minind] < np.min(parameters.pThreshold))[0][0]], color='firebrick',\n",
        "            label='pRest=%0.2f'%np.min(parameters.pThreshold))\n",
        "ax.legend()\n",
        "ax.set_xlabel(r'$log_{10}$ Velocity')\n",
        "ax.set_ylabel('PDF')\n"
      ],
      "metadata": {
        "id": "N_6lugst9IoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Watershed Region occupancy**\n",
        "\n",
        "We can also look at the timeseries over watershed regions and discard timepoints where the occupancy (bout) time at each instance is longer than a threshhold. This is already done within the ```findWatershedRegions``` function we used to obtain a watershed segmentation. I hardcoded the threshold to 2 within that function and is not contained in the parameters dict.\n",
        "(*this is partly because Gordon didn't pay me enough to do software development AND science, and partly because I am not a software developer!*)\n",
        "\n",
        "But we can always impose this post-hoc, and it is quite important to do it!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c6a7keTmKrSb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY-yeTfGNok7"
      },
      "source": [
        "# 9.&nbsp; Preliminary Analyses"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets load the wshedfile."
      ],
      "metadata": {
        "id": "-rptIzevk497"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wshedfile = hdf5storage.loadmat('%s/%s/zVals_wShed_groups.mat'%(parameters.projectPath, parameters.method))"
      ],
      "metadata": {
        "id": "1Wxu70xOk3_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ethograms"
      ],
      "metadata": {
        "id": "HQQXfodOTjqH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-m-s9GpYolw"
      },
      "source": [
        "We can now create ethograms using the watershed region time series created in the last step. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IjHvBQ2Begp"
      },
      "source": [
        "wregs = wshedfile['watershedRegions'].flatten()\n",
        "ethogram = np.zeros((wregs.max()+1, len(wregs)))\n",
        "\n",
        "for wreg in range(1, wregs.max()+1):\n",
        "  ethogram[wreg, np.where(wregs==wreg)[0]] = 1.0\n",
        "\n",
        "\n",
        "ethogram = np.split(ethogram.T, np.cumsum(wshedfile['zValLens'][0].flatten())[:-1])\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(20,10))\n",
        "\n",
        "for e, name, ax in zip(ethogram, wshedfile['zValNames'][0], axes.flatten()):\n",
        "  print(e.shape)\n",
        "  ax.imshow(e.T, aspect='auto', cmap=mmpy.gencmap())\n",
        "  ax.set_title(name[0][0])\n",
        "  ax.set_yticks([i for i in range(1, wregs.max()+1, 4)])\n",
        "  ax.set_yticklabels(['Region %i'%(j+1) for j in range(1, wregs.max()+1, 4)])\n",
        "\n",
        "  xticklocs = [1800*i for i in range(5)]\n",
        "  ax.set_xticks(xticklocs)\n",
        "  ax.set_xticklabels([j/(1800) for j in xticklocs])\n",
        "\n",
        "ax.set_xlabel('Time (min)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxrwhMtMaaCa"
      },
      "source": [
        "## Map Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q58dYlIODxJ"
      },
      "source": [
        "Run the below code to see the behavioral map in action. \n",
        "\n",
        "This may take **2 minutes** to run. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wshedfile = hdf5storage.loadmat('%s/%s/zVals_wShed_groups.mat'%(parameters.projectPath, parameters.method))\n",
        " \n",
        "try:\n",
        "    tqdm._instances.clear()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10,5))\n",
        "zValues = wshedfile['zValues']\n",
        "m = np.abs(zValues).max()\n",
        "\n",
        "\n",
        "sigma=1.0\n",
        "_, xx, density = mmpy.findPointDensity(zValues, sigma, 511, [-m-10, m+10])\n",
        "axes[0].imshow(density, cmap=mmpy.gencmap(), extent=(xx[0], xx[-1], xx[0], xx[-1]), origin='lower')\n",
        "axes[0].axis('off')\n",
        "axes[0].set_title('Method : %s'%parameters.method)\n",
        "sc = axes[0].scatter([],[],marker='o', color='k', s=500)\n",
        "\n",
        "h5ind = 0\n",
        "tstart = 2000\n",
        "connections = [[0, 2, 5, 11, 13, 14, 15 ,16], [0, 1, 4, 7, 10, 13], [0, 3, 6, 9, 12, 13], [1, 2, 3], [4, 5, 6], [7, 8, 9],\n",
        "                [10, 11, 12]]\n",
        "\n",
        "h5ind_wshed = np.argwhere(np.array([d[0][0].split('_pcaModes')[0] for d in wshedfile['zValNames'][0]])==datasetnames[h5ind])[0][0]\n",
        "def animate(t):\n",
        "    t = int(t*clips[h5ind].fps)+tstart\n",
        "    axes[1].clear()\n",
        "    im = axes[1].imshow(clips[h5ind].get_frame(t/clips[h5ind].fps), cmap='Greys', origin='lower')\n",
        "    for conn in connections:\n",
        "        axes[1].plot(h5s[h5ind][t, conn, 0], h5s[h5ind][t, conn, 1], 'r-')\n",
        "    axes[1].axis('off')\n",
        "    sc.set_offsets(zValues[9000*h5ind_wshed+t])\n",
        "    return mplfig_to_npimage(fig) #im, ax\n",
        "\n",
        "\n",
        "anim = VideoClip(animate, duration=20) # will throw memory error for more than 100.\n",
        "plt.close()\n",
        "anim.ipython_display(fps=15, loop=True, autoplay=True, maxduration=120)"
      ],
      "metadata": {
        "id": "ePLZWAM-f6xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create region videos\n",
        "\n"
      ],
      "metadata": {
        "id": "iWRss0HMefip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll try to create region videos - we'll pick contiguous time points that belong in one watershed region, and see what the animals is doing at those times by creating a movie.  "
      ],
      "metadata": {
        "id": "XzuQ3TLgcIDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This creates region videos for all the region. \n",
        "wmax = wshedfile['watershedRegions'].max()\n",
        "\n",
        "# Making region videos. This can take a few minutes depending on how many watershed regions there are. Takes ~1min per region.\n",
        "\n",
        "for i in range(wmax):\n",
        "  if os.path.exists(parameters.projectPath + '/%s/RegionVids%i/regions_%.3i.mp4' % (parameters.method, wmax, i+1)):\n",
        "    continue\n",
        "  demoutils.makeregionvideo_mice(i, parameters, h5s, clips, datasetnames, minLength=10, maxLength=200, subs=3)"
      ],
      "metadata": {
        "id": "oPAVaIKzY1m6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Behavior annotation"
      ],
      "metadata": {
        "id": "mtgmBUT1i59a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the region videos are finished running, look through them using this cell below."
      ],
      "metadata": {
        "id": "ExInuuOBixBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "\n",
        "outputdir = '%s/%s/RegionVids%i/'%(parameters.projectPath, parameters.method, wshedfile['watershedRegions'].max())\n",
        "wmax = wshedfile['watershedRegions'].max()\n",
        "\n",
        "\n",
        "def makeframe(Region):\n",
        "    mp4path = outputdir + 'regions_' + '%.3i' % (Region) + '.mp4'\n",
        "\n",
        "    if os.path.exists(mp4path):\n",
        "        mp4 = open(mp4path,'rb').read()\n",
        "        \n",
        "        data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "        display(HTML(\"\"\"\n",
        "        <video width=400 controls loop autoplay>\n",
        "            <source src=\"%s\" type=\"video/mp4\">\n",
        "        </video>\n",
        "        \"\"\" % data_url))\n",
        "    else:\n",
        "        print('No video found!')\n",
        "    \n",
        "isldr = ipywidgets.IntSlider\n",
        "sliders = {'Region':isldr(value=1, min=1, max=wshedfile['watershedRegions'].max(), layout=ipywidgets.Layout(width='600px')),}\n",
        "\n",
        "\n",
        "i = ipywidgets.interactive_output(makeframe, sliders)\n",
        "\n",
        "b = ipywidgets.HBox([sliders['Region']])\n",
        "\n",
        "display(b, i)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_9ihSVWxXe2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you scroll through region videos above,fill in the behavior dictionary below. Feel free to modify the keys as you like."
      ],
      "metadata": {
        "id": "XGtrWr_HjDrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "behavior_dict = {'right turn':[1,2], }\n",
        "                #  'locomotion': [], \n",
        "                #  'left turn' : [], \n",
        "                #  'grooming' : [],\n",
        "                #  'rearing' : [],\n",
        "                #  }\n",
        "\n",
        "def convert_LL_behlist(LL, bdict):\n",
        "    LL2 = np.zeros_like(LL, dtype='int')\n",
        "    LL2[LL>0] = 1\n",
        "    print(LL2.shape)\n",
        "    blist = np.empty((np.max(LL), 2), dtype=object)\n",
        "    blist[:,1] = ''\n",
        "    for k,v in bdict.items():\n",
        "        blist[np.array(v)-1,1] = k\n",
        "    blist[:,0] = np.arange(len(blist))+1\n",
        "    behs = np.array(np.setdiff1d(np.unique(blist[:,1]), ''))\n",
        "    for i in blist:\n",
        "        if not i[1]:\n",
        "            continue\n",
        "        LL2[LL==i[0]] = np.where(behs==i[1])[0][0]+2\n",
        "    return LL2, blist \n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "\n",
        "LL2, blist = convert_LL_behlist(wshedfile['LL'], behavior_dict)\n",
        "ax.imshow(LL2, origin='lower', cmap=mmpy.gencmap())\n",
        "ax.axis('off')\n",
        "\n",
        "fs = 25\n",
        "\n",
        "for bi,beh in enumerate(np.array(np.setdiff1d(np.unique(blist[:,1]), ''))):\n",
        "    x,y = np.where(LL2==bi+2)\n",
        "    \n",
        "    ax.scatter(y.mean(), x.mean(), c='k', marker='x')\n",
        "    ax.text(y.mean(),x.mean(),beh, fontdict={'fontname':'monospace', 'fontsize':fs, \n",
        "                                             'fontweight':'bold'})\n",
        "\n",
        "ax.axis('off')"
      ],
      "metadata": {
        "id": "64hriJsbbPOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, you know everything you need to know to create a behavioral map for a set of datasets and be able to actually start doing some science! \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8X9SvyLia_hT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also zip your project folder by calling \n",
        "```!zip -r Mice_mmpy.zip Mice_mmpy``` \n",
        "and download the folder on your local computer to play around with it. "
      ],
      "metadata": {
        "id": "7sHy6AF8YY25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r Mice_mmpy.zip Mice_mmpy"
      ],
      "metadata": {
        "id": "ySony2raayOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.&nbsp; Transitions Analysis\n",
        "\n",
        "We'll use the watershed region assignments for analyses in this section. Lets load them up."
      ],
      "metadata": {
        "id": "ffoAwjqZlxa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wregsall = wshedfile['watershedRegions'][0]\n",
        "print(wregsall.shape)"
      ],
      "metadata": {
        "id": "j_xpqT1cmRr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wregssplit = np.split(wregsall, np.cumsum(wshedfile['zValLens'][0])[:-1])\n",
        "print([w.shape for w in wregssplit])\n",
        "print([d[0][0] for d in wshedfile['zValNames'][0]])"
      ],
      "metadata": {
        "id": "aI1WgzSQmWnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transition Matrix"
      ],
      "metadata": {
        "id": "Url9oJaIlxN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 4, figsize=(16,5))\n",
        "\n",
        "transitions = demoutils.getTransitions(wregsall)\n",
        "statevals = np.unique(transitions).tolist()\n",
        "statevals = statevals + [statevals[-1]+1]\n",
        "for ax, d in zip(axes.flatten(), [1, 10, 100, 1000]):\n",
        "    F = demoutils.makeTransitionMatrix(transitions, d)\n",
        "    ax.imshow(F, cmap=mmpy.gencmap(), extent=(statevals[0], statevals[1], statevals[0], statevals[1]))\n",
        "    ax.set_xlabel('Initial State') \n",
        "    ax.set_ylabel('Final State')\n",
        "    ax.set_title(r'$\\tau = $%i'%d)"
      ],
      "metadata": {
        "id": "h3swt3dslxMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eigenvalues of Transition matrix at different delays"
      ],
      "metadata": {
        "id": "Xul66AVSnK0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fix, ax, eigs = demoutils.plotLaggedEigenvalues([demoutils.getTransitions(w) for w in wregssplit], \n",
        "                                                lags = np.arange(1, 11).tolist() + np.arange(15, 100, step=5).tolist())"
      ],
      "metadata": {
        "id": "4zjtAGDPlxKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END OF NOTEBOOK"
      ],
      "metadata": {
        "id": "_twbCv0yazsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## WORK IN PROGRESS\n",
        "\n",
        "- Show transitions on map. "
      ],
      "metadata": {
        "id": "-QgPR6PUnGmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NaysanSaran/markov-chain\n",
        "!cp markov-chain/src/markovchain.py . \n",
        "!cp markov-chain/src/node.py .\n"
      ],
      "metadata": {
        "id": "H_K7PhKknJLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import markovchain"
      ],
      "metadata": {
        "id": "no-7vV60nQBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = demoutils.makeTransitionMatrix(transitions, 1)\n",
        "\n",
        "P = np.array(T) # Transition matrix\n",
        "mc = markovchain.MarkovChain(P, wshedfile['LL'])\n",
        "mc.draw()"
      ],
      "metadata": {
        "id": "O8N-jwR5iD4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mc.nodes[0].label"
      ],
      "metadata": {
        "id": "pUXTMopbnV_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "\n",
        "# Set the axis limits\n",
        "ax.set_xlim(mc.xlim)\n",
        "ax.set_ylim(mc.ylim)\n",
        "\n",
        "# Draw the nodes\n",
        "for node in mc.nodes:\n",
        "    node.add_circle(ax)\n",
        "\n",
        "# Add the transitions\n",
        "for i in range(mc.M.shape[0]):\n",
        "    for j in range(mc.M.shape[1]):\n",
        "        # self loops\n",
        "        if i == j:\n",
        "            # Loop direction\n",
        "            if mc.nodes[i].y >= 0:\n",
        "                mc.nodes[i].add_self_loop(ax, prob = mc.M[i,j], direction='up')\n",
        "            else:\n",
        "                mc.nodes[i].add_self_loop(ax, prob = mc.M[i,j], direction='down')\n",
        "        # directed arrows\n",
        "        elif mc.M[i,j] > 0:\n",
        "            mc.add_arrow(ax, mc.nodes[i], mc.nodes[j], prob = mc.M[i,j])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ENSQ2Ig6lpo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots"
      ],
      "metadata": {
        "id": "hjxDwKuKnw0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LL = wshedfile['LL']\n",
        "i = 10\n",
        "%matplotlib widget\n",
        "fig, ax = plt.subplots()\n",
        "y,x = np.where(LL==i)\n",
        "y,x = y.mean(), x.mean()\n",
        "ax.imshow(LL, origin='lower')\n",
        "ax.scatter(x, y,color='white')\n"
      ],
      "metadata": {
        "id": "kU8tbvk0iIhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JAfPh5l_josj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0__-meXBjt_G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}